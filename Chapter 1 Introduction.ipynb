{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preface  \n",
    "不知不觉，在 Machine Learning 的领地里面兜兜转转也 6 年了。2014 年的时候， Machine Learning 还不是一个很火的话题，随后的 2015 年和 2016 年，Machine Learning 又或者说是 Aritificial Intelligence 成为了 Computer Science 领域一个火热的概念。但是到了 2017 年和 2018年，Machine Learning 的话题热度渐渐的冷却了下来，因为在很多实际的场景中， Machine Learning 的算法还不能够很好的落地。在 2010 年左右的时候，人们能读到的关于 Machine Learning 的内容大多都是 Statistical Machine Learning，和过去几十年相比，唯一变化的是数据量和处理数据的能力。最近 5 年，大家更多的提到的内容更多的是 Deep Learning，讲实话两者的实际区别，远比那些炒作的概念要小的多。当然也得承认的事情，Deep Learning 的“魔法”在很多的数据集和场景上得到了非常优秀的结果。\n",
    "\n",
    "这么多年过去了，觉得也就摸到了 Machine Learning 的门，应该谈不上进去吧。所以这个 Github 项目仅仅是 2017 年底开始的在公司内部分享的一个总结，希望有缘人看到了能对他有所帮助，如果发现相关的话题中出现错误，也请邮件告知我（linhaigu@gmail.com）。每一个话题之间尽量做到相互独立，把每一件事情都讲清楚。每一个算法都将提供两种实现方式，以便于加深理解：\n",
    "\n",
    "* 基于 Numpy 的原生实现。  \n",
    "* 基于开源机机器学习库调用的实现。\n",
    "\n",
    "同时，这个 repository 也会是一个笔记，记录自己学习、阅读和使用 Machine Learning 的过程。5 个月前，这个 Repository 里面的内容包含一些组织的分享的内容，有些内容现在已经过时了，趁这次的机会，会加入一些较新的内容。\n",
    "\n",
    "## 语言与工具  \n",
    "这个 Jupyter 中使用的编程语言主要是 Python。使用的额外的工具可能包括 PyTorch、Tensorflow等。\n",
    "\n",
    "## 参考资料  \n",
    "* [李航，统计学习方法](https://book.douban.com/subject/10590856/)\n",
    "* [周志华，机器学习](https://book.douban.com/subject/26708119/)\n",
    "* [Christopher Bishop](https://book.douban.com/subject/2061116/)\n",
    "* [Ian Goodfellow](https://book.douban.com/subject/26883982/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intruduction\n",
    "\n",
    "从我接触到的 Machine Learning 历史来看，Machine Learning 基本上是和 Computer Science 同步发展的。有些资料甚至把历史追溯到了 1642 年。真的要追溯到 1642 年的话就有些过分了，因为那个 19 岁的法国人创造出来的计算器并不能体现“学习”的过程。个人认为，1950s 作为 Machine Learning 的开端是合理的，在这个时代，人们开始用一些简单的算法做一些看起来“智能”的事情。也正是在那个时代的 1957 年，Frank Rosenblatt 发明了第一个体现“学习”的算法 Perceptron。就在发明 Perceptron 的前一年的夏天，John McCarthy，Marvin Minsky, Nathaniel Rochester 和 Claude Shannon 在达特茅斯召开了达特茅斯会议，会议的主要内容就是关于 “thinking machine” 方面的头脑风暴。很多人认为这个会议在“人工智能”的历史上有着重要的意义。有人常常会问，Machine Learning 和 Aritificial Intelligence 有什么区别，在我理解来看，Machine Learning 是实现 Aritificial Intelligence 的一种方法，也就是说实现 A.I. 有很多条路，ml 是其中的一条路而已。\n",
    "\n",
    "从维基百科上可以看到的 Machine Learning 历史：\n",
    "\n",
    "|时间|焦点\n",
    "| - | :-: |\n",
    "|1950s  |使用简单算法研究 Machine Learning 的时代 |\n",
    "|1960s\t|贝叶斯方法引入 Machine Learning |\n",
    "|1970s\t|Backpropagation 为 Machine Learning 注入活力 |\n",
    "|1980s\t|Machine Learning 的寒冬 |\n",
    "|1990s\t|从知识驱动转变为数据驱动；发明了 SVM 和 RNN |\n",
    "|2000s\t|核方法开始变的流行 |\n",
    "|2010s\t|Deep Learning 的时代 |\n",
    "\n",
    "上面的表格中有几个需要注意的时代：1970s 、1990s 、2010s。这三个时代典型的反应了神经网络的浪潮。特别要提到的是 1970s 年代，在那个时代的美国，很多研究人员乐观的相信在未来的 5 - 10 年类人们能完全解决“智能”的问题，很多的资金和人才都投入到到了 Machine Learning 领域，像极了 2010s 发生的事情，以现在的视角来看 1970s 的事情，他们的乐观就像个“笑话”了。也许同样在未来的 40 年后再看 2010s 这波神经网络浪潮也会觉得是一个 “笑话”。从现在来看当时的研究人员对与这个问题过于“乐观”了，直到现在，人们在应用比较成熟的 Machine Learning 的时候依然会面临数据质量、数据大小、计算能力，合适的模型之类的问题；而这些问题放在 1970s 甚至可以说是无法解决的，即使对于现在（2020年），上面提到的问题依然大量存在。几年之后在人们发现“智能”不太可能在短期内实现后，Aritificial Intelligence 的泡沫破灭了，从此 Aritificial Intelligence 也转向了寒冬，相关的创业企业大规模的倒闭，科研上投入的资金也大规模收缩了。\n",
    "\n",
    "Machine Learning  这发展的 60 多年里，人们提出了许多的 Machine Learning 算法。有基于规则推理的，研究人员通过创建一个可以不断增加知识的知识库，以及构建知识库上的推理引擎来实现所谓的“智能‘。这种方式遇到了很多的问题，其中一个令人烦恼的问题就是在遇到没有见过的知识的时候需要不断的向知识库中添加符合知识库表达的知识；另一个麻烦的事情就是你需要不断的修改推理引擎以使各种知识在协作的时候不至于互相矛盾，得到自己期望的结果。从 1990s 人们逐渐从基于规则推理的系统转换到数据驱动的算法上。基于数据的算法大多都是从给定的数据上学习得到一个给定统计分布的参数。从形式上来看，基于数据的算法更像是通过数据来自动的计算出统计模型的参数，至于事先确定的这个模型是否真的能反应数据本身，这无从得知。虽然 Machine Learning  驱动的方式改变了，但是利用统计方法研究 Machine Learning  的手段确一直保留了下来。如今人们经常使用的 Machine Learning 模型中，有很大一部分是通过统计理论支撑的。\n",
    "\n",
    "从 1990s 开始，主流是 SVMs 和 核方法，这个方法面对当时各种数据集都有不俗的表现，也正是 Support Vector Machine 的崛起吸引了大量的研究人员投入 Machine Learning 领域中。这个情况一直到 2006 年开始有所转变，那一年，Jeffery Hinton 发表了 Deep Belief Network，但是没办法通过训练得到最后的模型。当然，到现在为止，Hinton 。 Deep Belief Network 的最大的意义在于它提供了 Deep Learning 的训练方式，使得研究人员可以构建 Deep Learning 的模型。在 Hinton 之前，人们可以设计出 Deep Nerual Network，但是没办法通过训练得到最后的模型。当然，到现在为止，Hinton 的 Deep Belief Network 中的训练方法也已经被更加简单、直接和有效的方法所取代，现在得到的网络也越来越深，结构也越来越复杂，在不同问题的标准数据集上得到的效果也越来越好。正是这波 Deep Learning 的浪潮，又吸引一波、公司、进入到了 Machine Learning 领域中，到了 2020 年，很多纯粹靠炒作 Artificial Intelligence 的概念的公司，已经凉凉了。\n",
    "\n",
    "现在有很多人和很多资金投入到了这个领域中，有的人期望能将近年来在学术界表现出现的方法应用到实际的生活中，有些人期望能创造出更加“智能”的模型，解决之前没有解决的问题，很多人都对此抱有很高的期望。可是现实是残酷的，很多在标准数据集成功的方法，在实际生活中并不能很好的应用，而在学术界，自 2006 年的 Deep Learning 开始，革命性的变化也没有再出现。现在是 Machine Learning 又一个最好的时代，但这个泡沫正渐渐变小，人们似乎正在再次回归理性。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the Machine Learning\n",
    "\n",
    "前面的内容简要的讲了机器学习的历史和一些实际使用中的机器学习问题，但是至今为止，我都没有写下什么是机器学习。跟什么是 Aritificial Intelligence 不一样，机器学习是有一个比较权威的定义的：\n",
    "\n",
    "> 对于某类任务 T 和性能度量 P ，如果一个计算机程序在 T 上以 P 衡量的性能随着经验 E 而自我完善，那么我们称这个计算机程序从经验 E 中学习。 -- Tom Mitchell, Machine Learning\n",
    "\n",
    "上面的定义中 T 就是机器学习模型要解决的问题， P 就是衡量这个机器学习模型性能的评价方式， E 就是经常提到的数据。这个定义很好的体现了机器学习的过程及核心要素。\n",
    "\n",
    "这些年来，研究和工程人员一直致力于改善的就是这三个方面：\n",
    "\n",
    "1. 不断的创造新的模型解决新的问题。\n",
    "2. 不断的提出新的模型评价方式。\n",
    "3. 不断的增加用于训练的有效数据。\n",
    "\n",
    "从如今的具体实践来看，这样做很有效。上面的 1 和 2 适用于研究人员，而 3 则是适用于工程人员，创造新的模型和新的模型评价方式不是一件容易的事情，可是增加用于训练的有效数据往往就能直接的改善原有模型的效果。可换句话来说，这三个部分要付出的成本都很高，因为现在更多有效的模型就是 Supervised Learning ，大量的数据意味着需要大量对数据做出准确的标注，这需要花费很大的人力和时间的成本，当然这也就等价于付出了金钱的成本。现如今，很多公司（例如百度、Amazon、讯飞）构建了用于标注数据的数据标注平台，通过付出一定的金钱吸引人们用业余时间来标注公司自己或者他人上传的数据，以用于构建模型。近年来，研究人员也越来越多的从学术界转到拥有资金和数据的企业，存储和处理这些数据需要大量的机器和资金，这是学校往往不具备的，可是企业能提供这样的环境。\n",
    "\n",
    "## 机器学习的分类\n",
    "\n",
    "根据具体要处理的问题的类型，机器学习主要分成了两个大类：\n",
    "\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "\n",
    "每个大类之下又有一些具体的算法，这些具体的算法暂时不表，稍后随着章节的进行，可以自行进行体会。\n",
    "\n",
    "Supervised Learning 常常有人翻译成“监督学习”。在解释什么是 Supervised Learning 之前先得说明数据的表示。通常，我们使用（暂时的）行向量来表示一个数据，向量的每一个分量都称为一个 Feature，例如可以通过（姓名，性别，年龄，学历，家庭住址）来表示一个人，其中的每一个分量都是一个 Feature 。 如果这个时候，我们能认为这个向量表示一个人的收入的水平，即“收入=（姓名，性别，年龄，学历，家庭住址）”，那么“收入”这个变量就称之为 Label。**如果机器学习模型构建的是 Feature 和 Label 之间的关系，那么这个模型就属于 _Supervised Learning_**。\n",
    "\n",
    "类似的，Unsupervised Learning的解释就很容易了，**如果机器学习模型构建的数据上的输入只有 Feature 没有 Label，那么这个模型也就是发现数据之间的关系，这样的模型就属于 _Unsupervised Learning_**。\n",
    "\n",
    "有些资料会把 Label 表述成 Response Variable，会把 Feature 表输出 Attribute , 其实都是一个意思。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning in Real World\n",
    "\n",
    "欢迎来到现实世界！从如今的大多数实践来看，机器学习这项技术并没有很多的专家吹的那样神乎其神，当然那些专家讲过分夸张的话的时候的受众也不是实际使用机器学习技术的工程师。最近的几年，人们可以看到机 Machine Learning 在很多的__标准数据集__上的突破(2012 年 AlexNet 在 ImageNet 上的突破，2018 年 Bert 在 SOTA 上的突破)，这些当然是一件值得称赞和欣喜的事情，可是要注意的是，这些在__标准数据集__验证成功的技术，在真实的环境中往往不太能直接应用，经常会水土不服，简单的话说，就是实验室的结果和商业上想要的结果，其实存在很大的 Gap。以推荐系统举例，大多数书籍在讲到推荐的时候其实是构建在人与人，物品与物品间的相似性分析基础上，这是解决推荐的时候一个思路，而且大家基本都是这么做的。可是实际使用中，这个效果真的很好么？据我的认知，通过这种方式确实能提高一定的点击率，但是提升的非常的有限，能有 1% 的提升就已经能吹上天了。有些人会辩驳说，这 1% 带了可以带来很多的收益，但是得加个前提条件，这 1% 在很赚钱的业务上才能带来收益，否则连搭建推荐系统付出的各种成本都无法解决。然而实际中， 1% 都是一种奢望，百分之零点零几的“提升”随处可见，甚至有些时候还是负提升。有一种情形下很好解释负提升，比如在推荐电影的时候，有一个片子是最近非常火爆，口碑很好的电影，但是和这个输入到模型中的 Feature 毫无关系，这个时候模型基本不可能推荐这部电影，可是实际生活中，大多数人还是凑这个热闹的。而“凑热闹”这个概念，在没有大规模使用机器学习的技术之前，很多的系统就会把其作为一条规则用于推荐。\n",
    "\n",
    "当然，越来越多的领域开始应用 Machine Learning，不过大多是辅助性，而不是决定性的。在应用机器学习技术替换掉现有系统的过程中是逐步进行，反复迭代的。不过在确定使用 Machine Learning 时候，需要仔细的确认下面的几个条件是否能够满足：\n",
    "\n",
    "1. 处理海量高质量的数据的能力\n",
    "2. 数据的清洗和标注的能力\n",
    "3. 要解决的问题是否是定义良好\n",
    "4. 获取的数据能够解决问题\n",
    "5. 有合适的模型能够解决问题\n",
    "\n",
    "上面的 5 条，每一条都不是一件容易的事情。处理海量高质量的数据的能力，需要花费大量的时间、人力、金钱成本。近期机器学习的突破的很大一个点就是能提供给模型训练的高质量的数据越来越多，但是存储和处理海量数据本身就不是一件容易的事情，需要搭建专门的软硬件系统，同时也需要专业的开发人员使用这些软件完成任务。第二个让人头疼的事情就是数据的标注，随着模型的能处理的问题越来越复杂，标注的内容也越来越多了。比如在图像定位这个问题上，需要标注出图片的每一个感兴趣的区域的大小，内容，乍一看不麻烦，但是感兴趣区域的大小需要像素级别的标注，再加上海量的图片，这件事就麻烦了。第三个让人发愁的条件是要解决的问题是否是定义良好的，也就是说要应用机器学习技术来解决业务中实际遇到的问题，有很多时候要解决的问题其实是模糊的，但是现在的机器学习技术往往能解决的问题比较的单一，往往需要附加上其他的功能组件才能真正的对业务有用。第四个条件就很重要了，收集的数据能不能解决问题，这很关键，如果收集的数据都是高质量，但是对解决问题毫无帮助的话，收集再多的数据也没有意义。最后的一个条件对于大多数公司来说反而不在意了，简单的来说就是_没有条件_。很多的公司并不是研究机构，也就是说，它们不需要发明新的模型来解决问题，而是需要应用已有的模型能将其嵌入到自己的业务中就可以了，当然要做一些适当的修改。可是如果现有的模型没办法解决业务上的问题的话，通常的做法会是放弃，或者是强制使用某一个看起来 _比较好_ 的模型。\n",
    "\n",
    "Deep Learning 是一个很棒的工具和方法，但不是银弹。\n"
   ]
  }
 ]
}