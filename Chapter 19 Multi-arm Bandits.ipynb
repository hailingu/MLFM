{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Multi-arm Bandits\n",
    "\n",
    "假设有 10 个摇臂，在任意时刻 t 只能摇动其中的 1 个，每次摇动之后会获得一个反馈 r。那么问题就是，假如需要摇动 1000 次摇臂，如何使得最后累积的 r 的值最大呢？这就是 Multi-arm Bandits 问题。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1, 2, 9, 1, 9, 3, 4, 4, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 先创造 10 个随机 [1, 10] 之间的随机整数, 每个整数代表每个摇臂的期望收益\n",
    "import random\n",
    "\n",
    "average_rewards = [random.randint(1, 9) for _ in range(0, 10)]\n",
    "print(average_rewards)\n",
    "\n",
    "\n",
    "# 同时设定，每个摇臂收益满足正态分布，且方差为 1\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "source": [
    "## Action-value Methods\n",
    "\n",
    "这个方法每一个摇臂创建一个 Q(a) 函数，来衡量每一个摇臂摇动之后能获得的价值 r，a 代表具体是哪个摇臂，计算的方法是：\n",
    "\n",
    "$$\n",
    "    Q(a) = \\frac{\\sum_{i=1}^{t-1} r_i I(Action=a)}{\\sum_{i=1}^{t-1}I(Action=a)}\n",
    "$$\n",
    "\n",
    "应用这个方法，计算以下，上面的 10 个摇臂摇 1000 次以后的 Q 值。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3.022129494927139, 8.943006690133885, 2.9888146316010675, 1.9989780855226307, 6.002526873274321, 7.992837041004136, 4.9649240459189485, 4.038011320874071, 7.992750798949209, 1.0128159550409008]\n"
     ]
    }
   ],
   "source": [
    "Q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "for n in range(1, 1001):\n",
    "    for i in range(0, 10):\n",
    "        Q[i] = norm.rvs(average_rewards[i], 1) / n + (n - 1) * Q[i] / n \n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "source": [
    "上面的计算结果可以看出，最后得到的 Q 里面的值，很接近之前设定的每个摇臂的期望值。但是，如果需要在 1000 次行动之内把收益最大化，上面这种需要 $1000 \\times 10 + 1000$ 次的方法就不能使用了。\n",
    "\n",
    "如果我们用 50 次去探索每个摇臂的 Q 值，然后总是选择 Q 值最大的摇臂摇动，那么可以获得的收益是："
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8390.495376772951\n"
     ]
    }
   ],
   "source": [
    "Q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "total_reward = 0\n",
    "for n in range(1, 5):\n",
    "    for i in range(0, 10):\n",
    "        r = norm.rvs(average_rewards[i], 1)\n",
    "        Q[i] = r / n + (n - 1) * Q[i] / n\n",
    "        total_reward += r\n",
    "\n",
    "max_q = max(Q)\n",
    "total_reward += 950 * max_q\n",
    "print(total_reward)"
   ]
  },
  {
   "source": [
    "通过最初的设定，我们知道，理想中最大的回报是 9000。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 贪婪方法\n",
    "\n",
    "在 t 时刻，以 $1 - \\epsilon$ 的概率选择摇动在 t 时刻之前 Q(a) 最大的摇臂，同时以 $\\epsilon$ 的概率选择所有动作。假设这个 $\\epsilon = 0.1$，那么可以得到"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8351.656668955518\n"
     ]
    }
   ],
   "source": [
    "Q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "C = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    epsilon = 0.1\n",
    "    rand = random.random()\n",
    "    if rand < epsilon:\n",
    "        a = random.randint(0, 9)\n",
    "    else:\n",
    "        a = Q.index(max(Q))\n",
    "    \n",
    "    r = norm.rvs(average_rewards[a], 1)\n",
    "    total_reward += r\n",
    "    C[a] += 1\n",
    "    Q[a] = r / C[a] + (C[a] - 1) / C[a] * Q[a]\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "source": [
    "### 乐观的初始值\n",
    "\n",
    "即设置 Q 的初始值全部为 5，这样更加有利于去探索。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8582.209573379989\n"
     ]
    }
   ],
   "source": [
    "init = 5\n",
    "Q = [init, init, init, init, init, init, init, init, init, init]\n",
    "C = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "\n",
    "total_reward = 0\n",
    "for i in range(1, 1001):\n",
    "    epsilon = 0.1\n",
    "    rand = random.random()\n",
    "    if rand < epsilon:\n",
    "        a = random.randint(0, 9)\n",
    "    else:\n",
    "        a = Q.index(max(Q))\n",
    "    \n",
    "    r = norm.rvs(average_rewards[a], 1)\n",
    "    total_reward += r\n",
    "    C[a] += 1\n",
    "    Q[a] = r / C[a] + (C[a] - 1) / C[a] * Q[a]\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "source": [
    "### UCB\n",
    "\n",
    "该方法是另一种鼓励探索的设定。选择摇臂的价值时候，采用如下的公式评估摇臂的价值：\n",
    "\n",
    "$$\n",
    "A_t = argmax_{a} \\ [Q(a) + c \\cdot \\sqrt{\\frac{ln\\ t}{N_t(a)}}]\n",
    "$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8771.601918799377\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "c = 2\n",
    "Q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "C = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    A = [Q[a] + c * math.sqrt(math.log(i) / (C[a] + 1)) for a in range(0, 10)]\n",
    "    a = A.index(max(A))\n",
    "    r = norm.rvs(average_rewards[a], 1)\n",
    "    total_reward += r\n",
    "    C[a] += 1\n",
    "    Q[a] = r / C[a] + (C[a] - 1) / C[a] * Q[a]\n",
    "\n",
    "print(total_reward)"
   ]
  },
  {
   "source": [
    "### Gradient Bandit\n",
    "\n",
    "也可以使用梯度来计算收益，计算公式：\n",
    "\n",
    "$\\begin{align*}\n",
    "H_{t+1}(A_t) & = H_t(A_t) + \\alpha(R_t - \\overline{R_t})(1 - \\pi_t(A_t)) \\\\\n",
    "H_{t+1}(a) & = H_t(a) + \\alpha(R_t - \\overline{R_t})\\pi_t(a)\n",
    "\\end{align*}$\n",
    "\n",
    "上面的式子中 $H_{t}$ 初始化都是 0， $\\pi_t$ 的计算公式：\n",
    "\n",
    "$\\pi_t(a)=\\frac{e^{H_t{a}}}{\\sum_{b=1}^ke^{H_tb}}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "import math\n",
    "\n",
    "Q = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "C = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "H = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    h_sum = sum([math.exp(h) for h in H])\n",
    "    p = [math.exp(H[u]) / h_sum for u in range(0, 10)]\n",
    "    a = p.index(max(p))\n",
    "    r = norm.rvs(average_rewards[a], 1)\n",
    "    total_reward += r\n",
    "    r_t = total_reward / i\n",
    "    C[a] += 1\n",
    "    Q[a] = Q[a] + 1 / C[a] * (r - Q[a])\n",
    "    H[a] = H[a] + 0.1 * (r - r_t) * (1 - p[a])\n",
    "    TH = [H[q] - 0.1 * (r - r_t) * p[q] for q in range(0, 10)]\n",
    "    TH[a] = H[a]\n",
    "    H = TH\n",
    "\n",
    "print(total_reward)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 142,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "8968.005769935862\n"
     ]
    }
   ]
  }
 ]
}