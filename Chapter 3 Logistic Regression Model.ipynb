{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "和前面提到的 Linear Regression 一样， Logistic Regression 也属于 Generalized Linear Model。Logistic Regression 是 Linear Regression 很直接的扩展，Logistic Regression 把 Linear Regression 的结果送入到 sigmoid 函数中，计算得到结果。\n",
    "\n",
    "## sigmoid 函数\n",
    "\n",
    "$\\begin{align*}\n",
    "sigmoid(x) = \\frac{1}{1 + e^{-x}} = \\frac{e^x}{e^x + 1}\n",
    "\\end{align*}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "x = np.linspace(-10,10,200)\n",
    "y = 1 / (1 + np.exp(-1 * x))\n",
    "plt.plot(x, y, 'b', label='sigmoid(x) = e^x / (e^x + 1)')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('sigmoid(x)')\n",
    "plt.title('sigmoid(x) = e^x / (e^x + 1)')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "很多人常讲，Logistic Regression 最后得到的结果是一个概率值。这个值真的是概率值么？指数分布簇可以给我们答案。\n",
    "\n",
    "## Exponential Family\n",
    "\n",
    "单一变量的 exponential family 是 $f(x|\\theta)=h(x)e^{\\eta(\\theta)T(x)-A(\\theta)}$\n",
    "\n",
    "其中 $\\eta(\\theta)$ 是自然参数，在 Bernoulli 分布中，只有一个自然参数，那就是 $p$。另外的，$A(\\theta)$ 可以表示成 $A(\\theta) = f(\\eta(\\theta))$ 的形式。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 $x \\sim Bernoulli(x|p) = p^x(1-p)^{1-x}$，那么就有：\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "Bernoulli(x|p) & = p^x(1-p)^{1-x} \\\\\n",
    "& = e^{log(p^x(1-p)^{1-x})} \\\\\n",
    "& = e^{xlog(p) + (1-x)log(1-p)} \\\\ \n",
    "& = e^{xlog(\\frac{p}{1-p}) + log(1-p)}\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "对应指数分布簇，即有 $h(x)=1$，$\\eta(\\theta)=log(\\frac{p}{1-p})$，$T(x)=x$，$A(\\theta)=-log(1-p)$。稍微对 p 做一些分析：\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\eta(\\theta) & = log(\\frac{p}{1-p}) \\\\\n",
    "\\Rightarrow e^{\\eta(\\theta)} \\cdot (1-p) & = p \\\\\n",
    "e^{\\eta(\\theta)} & = p(1 + e^{\\eta(\\theta)}) \\\\\n",
    "\\frac{e^{\\eta(\\theta)}}{1+e^{\\eta(\\theta)}} & = p \n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "所以，如果使用 $\\eta(\\theta)) = \\mathbf{\\theta}^T \\mathbf{x}$，那么 $p = \\frac{1}{1+e^{-\\theta^T x}}$，所以有人会说 Logistic Regression 的输出是一个概率值。在传统的 Statistical Machine Learning 中，通过预先选择一个模型，然后根据数据，学习出模型的参数值。过去的一段时间内，我一度认为 Logistic Regression 的输出值，不能代表概率值，其实是部分正确的，如果说 $\\eta(\\theta)=\\mathbf{\\theta}^T \\mathbf{x}$ 能真实的反映 $\\eta(\\theta)$ 函数，那么 $p$ 的值就是概率值；否则就不行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似于 Linear Regression，要通过训练数据获取 $\\theta$ 的值，也需要类似的两步，设置 Loss 和使用 Learning Method。在 Logistic Regression 中，常用的损失函数称为 logloss 或者 cross-entropy，两者分别是 Statistics 和 Information Theory 视角的表述。\n",
    "\n",
    "## logloss\n",
    "\n",
    "简单的做一个推导\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "{logloss}_i = y_i \\cdot log(\\hat{y_i}) + (1 - y_i) \\cdot log(1 - \\hat{y_i})\n",
    "\\end{align*}\n",
    "$\n",
    "\n",
    "这样的话，如果 $y=0$，那么只有 $(1 - y) \\cdot log(1 - \\hat{y})$ 生效，且 $\\hat{y}$ 要趋近于 0 才能使得 loss 最小；如果 $y=1$，那么只有 $y \\cdot log(\\hat{y})$ 生效，且 $\\hat{y}$ 要趋近于 1 才能是 loss 最小。\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "logloss & = \\frac{1}{N} \\sum\\limits_i^N [y_i \\cdot log(\\hat{y_i}) + (1 - y_i) \\cdot log(1 - \\hat{y_i})] \\\\\n",
    "& \\Rightarrow \\frac{1}{N} \\sum\\limits_i^N  [y_i \\cdot log (\\frac{e^{\\theta^T \\mathbf{x_i}}}{1 + e^{\\theta^T \\mathbf{x_i}}}) + log (\\frac{1}{1 + e^{\\theta^T \\mathbf{x_i}}}) - y \\cdot log (\\frac{1}{1 + e^{\\theta^T \\mathbf{x_i}}})] \\\\\n",
    "& = \\frac{1}{N} \\sum\\limits_i^N [y_i \\cdot (\\theta^T \\mathbf{x_i}) - y_i \\cdot log(1 + e^{\\theta^T \\mathbf{x_i}}) - log(1 + e^{\\theta^T \\mathbf{x_i}}) + y_i \\cdot log(1 + e^{\\theta^T \\mathbf{x_i}})]\\\\\n",
    "& = \\frac{1}{N} \\sum\\limits_i^N [y_i \\cdot (\\theta^T \\mathbf{x_i}) - log(1 + e^{\\theta^T \\mathbf{x_i}})]\n",
    "\\end{align*}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于某一个参数 $\\theta_i$，计算 gradient：\n",
    "\n",
    "$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial loss}{\\theta_i} & = \\frac{1}{N} \\sum\\limits_i^N [y_i \\cdot x_i - \\frac{e^{\\theta^T \\mathbf{x_i}}}{1 + e^{\\theta^T \\mathbf{x_i}}} x_i] \\\\\n",
    "& = \\frac{1}{N} \\sum\\limits_i^N (y_i \\cdot x_i - \\hat{y_i} x_i) = \\frac{1}{N} \\sum\\limits_i^N (y_i - \\hat{y_i}) x_i\n",
    "\\end{align*}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 iris 数据集来进行 Logistic Regression 的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "index = np.where(y < 2)\n",
    "\n",
    "X = X[index]\n",
    "y = y[index]\n",
    "\n",
    "X = np.c_[X, np.ones(len(index[0]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "EPOCH: 0, loss: -2.370534\nEPOCH: 3000, loss: -1.964829\nEPOCH: 6000, loss: -2.511892\nEPOCH: 9000, loss: -2.162495\nEPOCH: 12000, loss: -2.855219\nEPOCH: 15000, loss: -2.525736\nEPOCH: 18000, loss: -3.211407\nEPOCH: 21000, loss: -3.219594\nEPOCH: 24000, loss: -2.564125\nEPOCH: 27000, loss: -3.675666\n[[-0.00920321]\n [ 0.51700381]\n [ 0.2214591 ]\n [ 0.84188067]\n [ 1.37935963]]\n"
    }
   ],
   "source": [
    "# numpy\n",
    "np_theta = np.random.randn(X.shape[1], 1)\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 30000\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X.shape[0], size=BATCH_SIZE)\n",
    "    sample_x = X[index].reshape(X.shape[1], BATCH_SIZE)\n",
    "    sample_y = y[index]\n",
    "\n",
    "    y_pred = 1 / (1 + np.exp(-1 * np.dot(np_theta.T, sample_x)))\n",
    "    logloss = np.sum(np.multiply(sample_y, np.log(y_pred)) + np.multiply((1 - sample_y), np.log(1 - y_pred))) / BATCH_SIZE\n",
    "\n",
    "    o = np.sum(np.multiply(sample_y - y_pred, sample_x), 1).reshape(X.shape[1], 1)\n",
    "    np_theta -= LEARNING_RATE * o / BATCH_SIZE\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, logloss))\n",
    "\n",
    "print(np_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "EPOCH: 0, loss: -6.026857\nEPOCH: 300, loss: -7.379591\nEPOCH: 600, loss: -7.419720\nEPOCH: 900, loss: -8.910971\nEPOCH: 1200, loss: -5.656529\nEPOCH: 1500, loss: -6.962538\nEPOCH: 1800, loss: -6.705369\nEPOCH: 2100, loss: -6.862127\nEPOCH: 2400, loss: -6.425948\nEPOCH: 2700, loss: -8.399547\ntensor([[0.3517],\n        [1.3085],\n        [0.3771],\n        [2.3578],\n        [1.0618]], dtype=torch.float64, requires_grad=True)\n"
    }
   ],
   "source": [
    "# PyTorch\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.double\n",
    "\n",
    "INPUT_DIMENSION, OUTPUT_DIMENSION = X.shape[1], 1\n",
    "\n",
    "theta = torch.randn(INPUT_DIMENSION, OUTPUT_DIMENSION, device=device, dtype=dtype, requires_grad=True)\n",
    "LEARNING_RATE = 1e-5\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "EPOCH = 3000\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X.shape[0], size=BATCH_SIZE)\n",
    "    sample_x = torch.from_numpy(X[index]).reshape(INPUT_DIMENSION, BATCH_SIZE)\n",
    "    sample_y = torch.from_numpy(y[index])\n",
    "\n",
    "    y_pred = 1 / (1 + torch.exp(-1 * theta.T.mm(sample_x)))\n",
    "    logloss = torch.sum(torch.mul(sample_y, torch.log(y_pred)) + torch.mul((1 - sample_y), torch.log(1 - y_pred))) / BATCH_SIZE\n",
    "\n",
    "    logloss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        theta -= LEARNING_RATE * theta.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        theta.grad.zero_()\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, logloss))\n",
    "\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUC\n",
    "\n",
    "评价一个二元分类器的好坏，可以使用 AUC，AUC 的值越接近于 1 越好。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "numpy version auc is 0.996400\nPyTorch version auc is 0.991600\n"
    }
   ],
   "source": [
    "# numpy score\n",
    "\n",
    "np_score = 1 / (1 + np.exp(-1 * np.dot(np_theta.T, X.T)))\n",
    "py_theta = np.array([[0.3517],\n",
    "                     [1.3085],\n",
    "                     [0.3771],\n",
    "                     [2.3578],\n",
    "                     [1.0618]])\n",
    "py_score = 1 / (1 + np.exp(-1 * np.dot(py_theta.T, X.T)))\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, np_score.T)\n",
    "print('numpy version auc is %f' % metrics.auc(fpr,tpr))\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y, py_score.T)\n",
    "print('PyTorch version auc is %f' % metrics.auc(fpr,tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}