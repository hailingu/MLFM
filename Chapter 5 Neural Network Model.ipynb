{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Network 近十年最有突破的技术，虽然不是万能的，但是在解决很多问题上，已经表现出了划时代的意义。Neural Network 由很多层 layer 组成，每一层 layer 又由很多个 cell 组成。一个 cell 可以是一个简单的 Logistic Regression Model 也可以是 Linear Regression Model，还可以是其他复杂的模型。最近很多年大家在做的事情主要就是改变 layer 的连接结构，改变 cell 所采用的模型，改变 Neural Network 所采用的损失函数。\n",
    "\n",
    "我个人觉得，近期 Neural Network 发展过程中最重要的一个技术，就是自动求导，大大的降低了 Neural Network 的实现成本，使得人们可以快速的实现自己的想法，验证最后的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Dnn(nn.Module):\n",
    "    def __init__(self, layers_parameter, epoch=100, learning_rate=1e-3, optimizer='SGD', verbose=False, batch_size=32):\n",
    "        super(Dnn, self).__init__()\n",
    "        self.layers_parameter = layers_parameter\n",
    "        self.epoch = epoch\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = nn.BCELoss(reduction='mean')\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.dnn = None\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.dnn is None:\n",
    "            self.dnn = nn.Sequential(nn.Linear(X.shape[1], self.layers_parameter[0]).double(),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.BatchNorm1d(self.layers_parameter[0]).double(),\n",
    "                                     nn.Linear(self.layers_parameter[0], self.layers_parameter[1]).double(),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.layers_parameter[1], self.layers_parameter[2]).double(),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.layers_parameter[2], self.layers_parameter[3]).double(),\n",
    "                                     nn.ReLU(),\n",
    "                                     nn.Linear(self.layers_parameter[3], 1).double(),\n",
    "                                     nn.Sigmoid())\n",
    "\n",
    "            if self.optimizer == 'Adam':\n",
    "                self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate, amsgrad=True)\n",
    "            elif self.optimizer == 'SGD':\n",
    "                self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate, weight_decay=1e-5, momentum=0.1,\n",
    "                                           nesterov=True)\n",
    "\n",
    "        for epoch in range(self.epoch):\n",
    "            start = 0\n",
    "            end = start + self.batch_size\n",
    "            while start < X.shape[0]:\n",
    "\n",
    "                if end >= X.shape[0]:\n",
    "                    end = X.shape[0]\n",
    "\n",
    "                X_batch = torch.from_numpy(X[start:end, :])\n",
    "                y_batch = torch.from_numpy(y[start:end]).reshape(1, end - start)\n",
    "                y_batch_pred = self.forward(X_batch).reshape(1, end - start)\n",
    "\n",
    "                loss = self.loss_fn(y_batch_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                start = end\n",
    "                end += self.batch_size\n",
    "\n",
    "            if self.verbose and epoch % (self.epoch / 20) == 0:\n",
    "                print('EPOCH: %d, loss: %f' % (epoch, loss))\n",
    "        return self\n",
    "\n",
    "    def forward(self, X):\n",
    "        if self.dnn is not None:\n",
    "            return self.dnn(X)\n",
    "        else:\n",
    "            print('You should fit first!')\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        if self.dnn is not None:\n",
    "            return self.dnn(X)\n",
    "        else:\n",
    "            print('You should fit first!')\n",
    "            return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        if self.dnn is not None:\n",
    "            out = self.dnn(X)\n",
    "            out[out >= 0.5] = 1.0\n",
    "            out[out < 0.5] = 0.0\n",
    "            return out\n",
    "        else:\n",
    "            print('You should fit first!')\n",
    "            return self\n"
   ]
  }
 ]
}