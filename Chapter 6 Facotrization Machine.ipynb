{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine\n",
    "\n",
    "之前的　Linear Regression 的形式是形如　$\\hat{y}= w^T x$，Factorization Machine 在　Linear Regression　的基础之上添加了所谓的交叉项，即　$w_{ij}x_ix_j$, 即　$\\hat{y} = w_1x_1 + \\ldots + w_nx_n + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j$，由于有些交叉项在实际中并不存在，所以使用矩阵分解的办法，用一个　$k \\times n$　的矩阵，从中任选两个向量 $v_i, v_j$　相乘作为系数，从　$n$　个向量中任选两个相乘构成的系数个数一共有 $C_n^2 = \\frac{n(n-1)}{2}$　个，刚好等于后面交叉项的数量。\n",
    "\n",
    "最后的交叉项可以写成　$\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j = \\frac{1}{2}(\\sum_{i=1}^n \\sum_{j = 1}^n w_{ij}x_ix_j - \\sum_i^n w_{ii}x_ix_i) = \\frac{1}{2} \\sum_{f}^k[(\\sum_i^n v_{fi}x_i)^2 - \\sum_i^nv_{fi}^2 x_i^2]$，降低计算复杂度。\n",
    "\n",
    "其中 $w = v \\times v$\n",
    "\n",
    "计算　Loss 采用的函数仍是　MSE，即　$loss = (\\hat{y} - y)^2$\n",
    "\n",
    "Gradient 的计算及更新：\n",
    "\n",
    "$\\begin{align*}\n",
    "w_i & = w_i - \\eta \\cdot (\\hat{y} - y) \\cdot x_i \\\\\n",
    "v_{fi} & = v_{fi} - \\eta \\cdot (\\hat{y} - y) \\cdot (x_i \\sum_j^n v_{fj}x_j - x_i^2v_{fi})\n",
    "\\end{align*}$\n",
    "\n",
    "Factorization Machine 是一个适用于回归场景的算法。为了演示这个算法，采用典型的　Boston Housing 的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boston housing \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "boston_df = pd.DataFrame(X, columns=boston.feature_names)\n",
    "\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].mean()) / boston_df[feature].std()\n",
    "\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].min()) / (boston_df[feature].max() - boston_df[feature].min())\n",
    "\n",
    "X = np.c_[boston_df.values, np.ones(X.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 126.073349\nEPOCH: 1, loss: 68.203864\nEPOCH: 2, loss: 203.760829\nEPOCH: 3, loss: 445.360716\nEPOCH: 4, loss: 1164.094017\nEPOCH: 5, loss: 289.445419\nEPOCH: 6, loss: 436.679058\nEPOCH: 7, loss: 112.984207\nEPOCH: 8, loss: 196.410868\nEPOCH: 9, loss: 177.444503\nEPOCH: 10, loss: 298.400532\nEPOCH: 11, loss: 734.829578\nEPOCH: 12, loss: 91.091435\nEPOCH: 13, loss: 357.908206\nEPOCH: 14, loss: 409.608729\nEPOCH: 15, loss: 471.326977\nEPOCH: 16, loss: 365.820962\nEPOCH: 17, loss: 350.331697\nEPOCH: 18, loss: 2280.129894\nEPOCH: 19, loss: 1258.725440\n"
    }
   ],
   "source": [
    "k = 20\n",
    "\n",
    "LEARNING_RATE = 1e-5\n",
    "EPOCH = 20\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PRINT_NUMS = 20\n",
    "PRINT_INTERVAL = EPOCH / PRINT_NUMS\n",
    "\n",
    "n = X.shape[1] - 1\n",
    "\n",
    "w = np.random.uniform(0, 1, size=(1, n + 1))\n",
    "v = np.random.uniform(0, 1, size=(k, n))\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X.shape[0], size=BATCH_SIZE)\n",
    "    sample_x = X[index]\n",
    "    sample_y = y[index]\n",
    "\n",
    "    # linear part\n",
    "    linear_part = np.dot(w, sample_x.T)\n",
    "\n",
    "    # cross part\n",
    "    cross_part = 0\n",
    "    for f in range(k):\n",
    "        part_one = 0\n",
    "        part_two = 0\n",
    "        for i in range(n):\n",
    "            part_one = v[f,i] * sample_x[0,i]\n",
    "            part_two = v[f,i]** 2 * sample_x[0,i] ** 2\n",
    "\n",
    "        cross_part += (part_one - part_two) \n",
    "    cross_part = cross_part / 2\n",
    "\n",
    "    y_hat = linear_part +cross_part\n",
    "    loss = y_hat - sample_y\n",
    "\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, loss**2))\n",
    "\n",
    "    # update gradient\n",
    "    for i in range(n):\n",
    "        w[0,i] = w[0,i] - LEARNING_RATE * loss * sample_x[0,i]\n",
    "        \n",
    "        for f in range(k):\n",
    "            sum_part = 0\n",
    "            for j in range(n):\n",
    "                sum_part += v[f,j] * sample_x[0,j]\n",
    "            v[f,i] = v[f,i] - LEARNING_RATE * loss * (sample_x[0,i] * sum_part - v[f,i] * sample_x[0,i]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitmlfmconda074209e3e26947db8e6ceda371beb7cc",
   "display_name": "Python 3.6.10 64-bit ('mlfm': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}