{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine\n",
    "\n",
    "之前的　Linear Regression 的形式是形如　$\\hat{y}= w^T x$，Factorization Machine 在　Linear Regression　的基础之上添加了所谓的交叉项，即　$w_{ij}x_ix_j$, 即　$\\hat{y} = w_1x_1 + \\ldots + w_nx_n + \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j$，由于有些交叉项在实际中并不存在，所以使用矩阵分解的办法，用一个　$k \\times n$　的矩阵，从中任选两个向量 $v_i, v_j$　相乘作为系数，从　$n$　个向量中任选两个相乘构成的系数个数一共有 $C_n^2 = \\frac{n(n-1)}{2}$　个，刚好等于后面交叉项的数量。\n",
    "\n",
    "最后的交叉项可以写成　$\\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} w_{ij}x_ix_j = \\frac{1}{2}(\\sum_{i=1}^n \\sum_{j = 1}^n w_{ij}x_ix_j - \\sum_i^n w_{ii}x_ix_i) = \\frac{1}{2} \\sum_{f}^k[(\\sum_i^n v_{fi}x_i)^2 - \\sum_i^nv_{fi}^2 x_i^2]$，降低计算复杂度。\n",
    "\n",
    "其中 $w = v \\times v$\n",
    "\n",
    "计算　Loss 采用的函数仍是　MSE，即　$loss = (\\hat{y} - y)^2$\n",
    "\n",
    "Gradient 的计算及更新：\n",
    "\n",
    "$\\begin{align*}\n",
    "w_i & = w_i - \\eta \\cdot (\\hat{y} - y) \\cdot x_i \\\\\n",
    "v_{fi} & = v_{fi} - \\eta \\cdot (\\hat{y} - y) \\cdot (x_i \\sum_j^n v_{fj}x_j - x_i^2v_{fi})\n",
    "\\end{align*}$\n",
    "\n",
    "Factorization Machine 是一个适用于回归场景的算法。为了演示这个算法，采用典型的　Boston Housing 的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boston housing \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "boston_df = pd.DataFrame(X, columns=boston.feature_names)\n",
    "\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].mean()) / boston_df[feature].std()\n",
    "\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].min()) / (boston_df[feature].max() - boston_df[feature].min())\n",
    "\n",
    "X = np.c_[boston_df.values, np.ones(X.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 33.503794\nEPOCH: 50, loss: 233.236946\nEPOCH: 100, loss: 245.067962\nEPOCH: 150, loss: 139.987846\nEPOCH: 200, loss: 474.063754\nEPOCH: 250, loss: 338.626854\nEPOCH: 300, loss: 4.999315\nEPOCH: 350, loss: 335.814553\nEPOCH: 400, loss: 412.691163\nEPOCH: 450, loss: 422.737128\nEPOCH: 500, loss: 244.581224\nEPOCH: 550, loss: 59.741663\nEPOCH: 600, loss: 389.765044\nEPOCH: 650, loss: 3.832139\nEPOCH: 700, loss: 407.508306\nEPOCH: 750, loss: 368.649035\nEPOCH: 800, loss: 981.710612\nEPOCH: 850, loss: 61.615576\nEPOCH: 900, loss: 162.001958\nEPOCH: 950, loss: 1919.405309\n"
    }
   ],
   "source": [
    "k = 6\n",
    "\n",
    "LEARNING_RATE = 1e-7\n",
    "EPOCH = 1000\n",
    "\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "PRINT_NUMS = 20\n",
    "PRINT_INTERVAL = EPOCH / PRINT_NUMS\n",
    "\n",
    "n = X.shape[1] - 1\n",
    "\n",
    "w = np.random.uniform(0, 1, size=(1, n + 1))\n",
    "v = np.random.uniform(0, 1, size=(k, n))\n",
    "\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X.shape[0], size=BATCH_SIZE)\n",
    "    sample_x = X[index]\n",
    "    sample_y = y[index]\n",
    "\n",
    "    # linear part\n",
    "    linear_part = np.dot(w, sample_x.T)\n",
    "\n",
    "    # cross part\n",
    "    cross_part = 0\n",
    "    for f in range(k):\n",
    "        part_one = 0\n",
    "        part_two = 0\n",
    "        for i in range(n):\n",
    "            part_one = v[f,i] * sample_x[0,i]\n",
    "            part_two = (v[f,i]** 2) * (sample_x[0,i] ** 2)\n",
    "\n",
    "        cross_part += (part_one - part_two) \n",
    "    cross_part /= 2\n",
    "\n",
    "    y_hat = linear_part + cross_part\n",
    "    loss = y_hat - sample_y\n",
    "\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, loss**2))\n",
    "\n",
    "    # update gradient\n",
    "    w = w - LEARNING_RATE * loss * sample_x\n",
    "    for i in range(n):\n",
    "        for f in range(k):\n",
    "            sum_part = 0\n",
    "            for j in range(n):\n",
    "                sum_part += v[f,j] * sample_x[0,j]\n",
    "            v[f,i] = v[f,i] - LEARNING_RATE * loss * (sample_x[0,i] * sum_part - v[f,i] * sample_x[0,i]**2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bitmlfmconda074209e3e26947db8e6ceda371beb7cc",
   "display_name": "Python 3.6.10 64-bit ('mlfm': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}