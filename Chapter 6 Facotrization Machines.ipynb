{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine\n",
    "\n",
    "之前的　Linear Regression 的形式是形如　$\\hat{y}= w^T x$，Factorization Machine 在　Linear Regression　的基础之上添加了所谓的交叉项，即　$c_{ij}x_ix_j$, 即　$\\hat{y} = w_1x_1 + \\ldots + w_fx_f + \\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q$，由于有些交叉项在实际中并不存在，所以使用向量相乘的办法，用一个　$f \\times k$　的矩阵，从中任选两个向量 $v_i, v_j$　相乘作为系数，从　$n$　个向量中任选两个相乘构成的系数个数一共有 $C_f^2 = \\frac{f(f-1)}{2}$　个，刚好等于后面交叉项的数量。\n",
    "\n",
    "最后的交叉项可以写成　$\\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q = \\frac{1}{2}(\\sum_{p=1}^f \\sum_{q=1}^f c_{pq}x_px_q - \\sum_p^f v_p^2x_p^2)=\\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)(\\sum_{q=1}^f v_{q,u}x_q) - (\\sum_{p=1}^f v_{p, u}^2x_p^2)]= \\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)^2- (\\sum_{p=1}^f v_{p, u}^2x_p^2)]$，降低计算复杂度。\n",
    "\n",
    "其中 $c_{pq} = v_p \\cdot v_q$\n",
    "\n",
    "计算　Loss 采用的函数仍是　MSE，即　$loss = \\frac{1}{2} \\sum_i^n(\\hat{y}_i - y_i)^2$\n",
    "\n",
    "Gradient 的计算及更新：\n",
    "\n",
    "$\\begin{align*}\n",
    "w_i & = w_i - \\eta \\cdot \\frac{1}{n} \\cdot [\\sum_i^n x_i \\cdot (\\hat{y_i} - y_i) ] \\\\\n",
    "v_{p,u} & = v_{p,u} - \\eta \\cdot \\frac{1}{n} \\cdot \\sum_{i=1}^n [(\\hat{y_i} - y_i) \\cdot x_{i, p}^2 \\cdot (\\sum_{p=1}^f v_{p, u} - v_{p, u})]\n",
    "\\end{align*}$\n",
    "\n",
    "Factorization Machine 是一个适用于回归场景的算法（分类场景也可以使用）。为了演示这个算法，采用 Boston Housing 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "boston_data = pd.DataFrame(boston.data,  columns=boston.feature_names)\n",
    "boston_data['bias'] = np.ones(boston.data.shape[0])\n",
    "boston_data['target'] = boston.target\n",
    "\n",
    "ss = StandardScaler()\n",
    "boston_data = ss.fit_transform(boston_data)\n",
    "\n",
    "shape = boston_data.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston_data[0:shape[0], 0:-1], boston_data[0:shape[0], -1], test_size=0.25,\n",
    "                                                    random_state=33)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "ss: 2421.068171\nEPOCH: 334, loss: 2943.515062\nEPOCH: 335, loss: 17877.261137\nEPOCH: 336, loss: 8889.630842\nEPOCH: 337, loss: 2661.412424\nEPOCH: 338, loss: 629.099950\nEPOCH: 339, loss: 10880.182918\nEPOCH: 340, loss: 4256.975474\nEPOCH: 341, loss: 5394.365113\nEPOCH: 342, loss: 1482.767862\nEPOCH: 343, loss: 12826.270713\nEPOCH: 344, loss: 1095.472496\nEPOCH: 345, loss: 4635.479408\nEPOCH: 346, loss: 9378.882344\nEPOCH: 347, loss: 6092.925269\nEPOCH: 348, loss: 14057.590136\nEPOCH: 349, loss: 3319.328224\nEPOCH: 350, loss: 1172.559911\nEPOCH: 351, loss: 5595.512883\nEPOCH: 352, loss: 20884.348089\nEPOCH: 353, loss: 4007.364540\nEPOCH: 354, loss: 9719.476123\nEPOCH: 355, loss: 6248.360166\nEPOCH: 356, loss: 17496.993057\nEPOCH: 357, loss: 13018.473582\nEPOCH: 358, loss: 636.394697\nEPOCH: 359, loss: 2034.323169\nEPOCH: 360, loss: 17326.999071\nEPOCH: 361, loss: 7617.521790\nEPOCH: 362, loss: 4921.514630\nEPOCH: 363, loss: 3449.532465\nEPOCH: 364, loss: 716.568809\nEPOCH: 365, loss: 2264.796892\nEPOCH: 366, loss: 8119.784240\nEPOCH: 367, loss: 4306.848221\nEPOCH: 368, loss: 7309.474555\nEPOCH: 369, loss: 4478.661390\nEPOCH: 370, loss: 637.899539\nEPOCH: 371, loss: 2905.983644\nEPOCH: 372, loss: 4522.566905\nEPOCH: 373, loss: 3859.518643\nEPOCH: 374, loss: 2798.100431\nEPOCH: 375, loss: 6576.350163\nEPOCH: 376, loss: 4192.809987\nEPOCH: 377, loss: 7180.051522\nEPOCH: 378, loss: 15666.730222\nEPOCH: 379, loss: 2791.877513\nEPOCH: 380, loss: 2859.550959\nEPOCH: 381, loss: 1973.378073\nEPOCH: 382, loss: 5670.573982\nEPOCH: 383, loss: 3216.235413\nEPOCH: 384, loss: 6621.128547\nEPOCH: 385, loss: 4948.115294\nEPOCH: 386, loss: 5320.476874\nEPOCH: 387, loss: 2431.676734\nEPOCH: 388, loss: 1200.832727\nEPOCH: 389, loss: 4436.845888\nEPOCH: 390, loss: 1970.039018\nEPOCH: 391, loss: 1050.977543\nEPOCH: 392, loss: 9759.334993\nEPOCH: 393, loss: 2415.007459\nEPOCH: 394, loss: 4291.806674\nEPOCH: 395, loss: 3021.583219\nEPOCH: 396, loss: 9895.555570\nEPOCH: 397, loss: 1046.161023\nEPOCH: 398, loss: 3303.202321\nEPOCH: 399, loss: 2852.012886\nEPOCH: 400, loss: 3356.121826\nEPOCH: 401, loss: 31323.869767\nEPOCH: 402, loss: 3193.037113\nEPOCH: 403, loss: 3930.074229\nEPOCH: 404, loss: 2726.116710\nEPOCH: 405, loss: 7230.203510\nEPOCH: 406, loss: 2528.429206\nEPOCH: 407, loss: 3534.825549\nEPOCH: 408, loss: 2847.510619\nEPOCH: 409, loss: 4311.769755\nEPOCH: 410, loss: 3367.500319\nEPOCH: 411, loss: 4241.767268\nEPOCH: 412, loss: 1781.109093\nEPOCH: 413, loss: 3025.250896\nEPOCH: 414, loss: 4625.804427\nEPOCH: 415, loss: 24405.581329\nEPOCH: 416, loss: 4031.338696\nEPOCH: 417, loss: 3279.457984\nEPOCH: 418, loss: 5669.264258\nEPOCH: 419, loss: 1197.720508\nEPOCH: 420, loss: 3891.082086\nEPOCH: 421, loss: 6727.068056\nEPOCH: 422, loss: 2988.004214\nEPOCH: 423, loss: 4890.169682\nEPOCH: 424, loss: 1075.105189\nEPOCH: 425, loss: 2518.298585\nEPOCH: 426, loss: 16070.239974\nEPOCH: 427, loss: 4699.490214\nEPOCH: 428, loss: 1164.565971\nEPOCH: 429, loss: 12115.079657\nEPOCH: 430, loss: 7211.063504\nEPOCH: 431, loss: 12141.374343\nEPOCH: 432, loss: 1133.727370\nEPOCH: 433, loss: 1228.213592\nEPOCH: 434, loss: 1161.023960\nEPOCH: 435, loss: 5453.733001\nEPOCH: 436, loss: 3729.030697\nEPOCH: 437, loss: 3549.736035\nEPOCH: 438, loss: 7226.894170\nEPOCH: 439, loss: 5274.226467\nEPOCH: 440, loss: 5514.565406\nEPOCH: 441, loss: 1531.840695\nEPOCH: 442, loss: 6526.154923\nEPOCH: 443, loss: 503.445858\nEPOCH: 444, loss: 3248.258790\nEPOCH: 445, loss: 2727.932639\nEPOCH: 446, loss: 3026.933470\nEPOCH: 447, loss: 9761.033951\nEPOCH: 448, loss: 2333.171790\nEPOCH: 449, loss: 4072.804936\nEPOCH: 450, loss: 7247.969026\nEPOCH: 451, loss: 6392.158078\nEPOCH: 452, loss: 1081.245812\nEPOCH: 453, loss: 9163.931897\nEPOCH: 454, loss: 3076.041592\nEPOCH: 455, loss: 1777.535431\nEPOCH: 456, loss: 1282.368534\nEPOCH: 457, loss: 5474.390660\nEPOCH: 458, loss: 1442.606993\nEPOCH: 459, loss: 18307.891893\nEPOCH: 460, loss: 3202.726248\nEPOCH: 461, loss: 1124.497330\nEPOCH: 462, loss: 2533.828035\nEPOCH: 463, loss: 1670.235505\nEPOCH: 464, loss: 4286.471591\nEPOCH: 465, loss: 3259.733545\nEPOCH: 466, loss: 13514.122088\nEPOCH: 467, loss: 3569.669279\nEPOCH: 468, loss: 20419.533605\nEPOCH: 469, loss: 356.008218\nEPOCH: 470, loss: 11158.241012\nEPOCH: 471, loss: 2308.619892\nEPOCH: 472, loss: 5984.054562\nEPOCH: 473, loss: 3148.284056\nEPOCH: 474, loss: 9226.367601\nEPOCH: 475, loss: 2658.758446\nEPOCH: 476, loss: 8704.055083\nEPOCH: 477, loss: 2821.438386\nEPOCH: 478, loss: 5846.322784\nEPOCH: 479, loss: 6075.651468\nEPOCH: 480, loss: 1058.675511\nEPOCH: 481, loss: 6929.983341\nEPOCH: 482, loss: 2976.746114\nEPOCH: 483, loss: 3899.029446\nEPOCH: 484, loss: 4295.967265\nEPOCH: 485, loss: 19192.452007\nEPOCH: 486, loss: 3209.570096\nEPOCH: 487, loss: 4581.197214\nEPOCH: 488, loss: 1978.958193\nEPOCH: 489, loss: 10491.893335\nEPOCH: 490, loss: 8733.319578\nEPOCH: 491, loss: 416.390474\nEPOCH: 492, loss: 1781.682693\nEPOCH: 493, loss: 2421.676744\nEPOCH: 494, loss: 6779.398037\nEPOCH: 495, loss: 5968.158435\nEPOCH: 496, loss: 1771.914758\nEPOCH: 497, loss: 4165.904904\nEPOCH: 498, loss: 2372.569626\nEPOCH: 499, loss: 8359.215808\nEPOCH: 500, loss: 2182.541232\nEPOCH: 501, loss: 9036.083663\nEPOCH: 502, loss: 4498.637011\nEPOCH: 503, loss: 2380.530630\nEPOCH: 504, loss: 4790.169765\nEPOCH: 505, loss: 383.104077\nEPOCH: 506, loss: 2920.966356\nEPOCH: 507, loss: 3179.367660\nEPOCH: 508, loss: 2433.874212\nEPOCH: 509, loss: 1741.162787\nEPOCH: 510, loss: 5788.440456\nEPOCH: 511, loss: 2304.469419\nEPOCH: 512, loss: 5953.777109\nEPOCH: 513, loss: 4934.465465\nEPOCH: 514, loss: 4514.396317\nEPOCH: 515, loss: 6219.267977\nEPOCH: 516, loss: 11250.641119\nEPOCH: 517, loss: 2980.407151\nEPOCH: 518, loss: 5180.432639\nEPOCH: 519, loss: 1302.571104\nEPOCH: 520, loss: 12784.493253\nEPOCH: 521, loss: 3343.238033\nEPOCH: 522, loss: 3403.095068\nEPOCH: 523, loss: 2471.876360\nEPOCH: 524, loss: 3964.479773\nEPOCH: 525, loss: 3760.166115\nEPOCH: 526, loss: 11121.531476\nEPOCH: 527, loss: 4642.005418\nEPOCH: 528, loss: 3933.122237\nEPOCH: 529, loss: 2448.310916\nEPOCH: 530, loss: 348.099252\nEPOCH: 531, loss: 1390.520611\nEPOCH: 532, loss: 5130.164052\nEPOCH: 533, loss: 9119.369848\nEPOCH: 534, loss: 4600.829571\nEPOCH: 535, loss: 1846.854044\nEPOCH: 536, loss: 1395.393270\nEPOCH: 537, loss: 1747.173130\nEPOCH: 538, loss: 3331.080350\nEPOCH: 539, loss: 1171.644770\nEPOCH: 540, loss: 695.872097\nEPOCH: 541, loss: 1364.065887\nEPOCH: 542, loss: 14761.108834\nEPOCH: 543, loss: 2956.398929\nEPOCH: 544, loss: 6184.422293\nEPOCH: 545, loss: 563.720878\nEPOCH: 546, loss: 2795.889728\nEPOCH: 547, loss: 13116.233215\nEPOCH: 548, loss: 7965.119255\nEPOCH: 549, loss: 5678.348333\nEPOCH: 550, loss: 1012.520753\nEPOCH: 551, loss: 965.561009\nEPOCH: 552, loss: 2881.461190\nEPOCH: 553, loss: 3357.193513\nEPOCH: 554, loss: 1902.194179\nEPOCH: 555, loss: 3681.889999\nEPOCH: 556, loss: 174.000473\nEPOCH: 557, loss: 6902.363350\nEPOCH: 558, loss: 4378.215316\nEPOCH: 559, loss: 4191.147442\nEPOCH: 560, loss: 5749.661853\nEPOCH: 561, loss: 1316.955926\nEPOCH: 562, loss: 2194.400120\nEPOCH: 563, loss: 2439.752060\nEPOCH: 564, loss: 7880.124976\nEPOCH: 565, loss: 1344.040446\nEPOCH: 566, loss: 3020.235346\nEPOCH: 567, loss: 1576.110134\nEPOCH: 568, loss: 554.080268\nEPOCH: 569, loss: 9169.936866\nEPOCH: 570, loss: 1850.540790\nEPOCH: 571, loss: 3695.585262\nEPOCH: 572, loss: 2417.301427\nEPOCH: 573, loss: 1308.359919\nEPOCH: 574, loss: 5889.914541\nEPOCH: 575, loss: 5614.929757\nEPOCH: 576, loss: 10430.523265\nEPOCH: 577, loss: 2560.611600\nEPOCH: 578, loss: 4115.138773\nEPOCH: 579, loss: 2000.856907\nEPOCH: 580, loss: 8242.173046\nEPOCH: 581, loss: 994.083169\nEPOCH: 582, loss: 9135.559516\nEPOCH: 583, loss: 3961.920011\nEPOCH: 584, loss: 3290.322057\nEPOCH: 585, loss: 1090.154954\nEPOCH: 586, loss: 3139.422562\nEPOCH: 587, loss: 1849.558933\nEPOCH: 588, loss: 3535.791708\nEPOCH: 589, loss: 7282.726698\nEPOCH: 590, loss: 857.658067\nEPOCH: 591, loss: 1239.161922\nEPOCH: 592, loss: 886.264365\nEPOCH: 593, loss: 3087.591327\nEPOCH: 594, loss: 9965.520493\nEPOCH: 595, loss: 2418.133975\nEPOCH: 596, loss: 3093.803049\nEPOCH: 597, loss: 2725.297648\nEPOCH: 598, loss: 4115.719199\nEPOCH: 599, loss: 5605.371167\nEPOCH: 600, loss: 4159.399674\nEPOCH: 601, loss: 3752.003143\nEPOCH: 602, loss: 5580.210560\nEPOCH: 603, loss: 3718.723969\nEPOCH: 604, loss: 4218.420104\nEPOCH: 605, loss: 4725.837600\nEPOCH: 606, loss: 3035.136918\nEPOCH: 607, loss: 2691.312727\nEPOCH: 608, loss: 14705.446817\nEPOCH: 609, loss: 3739.858788\nEPOCH: 610, loss: 2348.833275\nEPOCH: 611, loss: 5347.220259\nEPOCH: 612, loss: 5413.181780\nEPOCH: 613, loss: 1219.603825\nEPOCH: 614, loss: 4434.159140\nEPOCH: 615, loss: 1347.911506\nEPOCH: 616, loss: 1451.765809\nEPOCH: 617, loss: 5305.309038\nEPOCH: 618, loss: 3934.320165\nEPOCH: 619, loss: 1867.669446\nEPOCH: 620, loss: 2100.711322\nEPOCH: 621, loss: 4421.718805\nEPOCH: 622, loss: 4304.741252\nEPOCH: 623, loss: 3686.840284\nEPOCH: 624, loss: 2992.543123\nEPOCH: 625, loss: 2002.593540\nEPOCH: 626, loss: 1482.770211\nEPOCH: 627, loss: 7487.242676\nEPOCH: 628, loss: 3584.757680\nEPOCH: 629, loss: 3046.833432\nEPOCH: 630, loss: 879.865104\nEPOCH: 631, loss: 1756.333717\nEPOCH: 632, loss: 4970.423882\nEPOCH: 633, loss: 2310.147886\nEPOCH: 634, loss: 3870.569290\nEPOCH: 635, loss: 4435.182952\nEPOCH: 636, loss: 9192.292948\nEPOCH: 637, loss: 9728.640309\nEPOCH: 638, loss: 1244.256795\nEPOCH: 639, loss: 5191.934389\nEPOCH: 640, loss: 1112.062378\nEPOCH: 641, loss: 2447.882124\nEPOCH: 642, loss: 2947.829377\nEPOCH: 643, loss: 5283.875017\nEPOCH: 644, loss: 3920.363659\nEPOCH: 645, loss: 4135.580964\nEPOCH: 646, loss: 3634.221815\nEPOCH: 647, loss: 1998.555235\nEPOCH: 648, loss: 894.111281\nEPOCH: 649, loss: 1934.623856\nEPOCH: 650, loss: 2762.743972\nEPOCH: 651, loss: 1249.924793\nEPOCH: 652, loss: 2646.178859\nEPOCH: 653, loss: 7284.416743\nEPOCH: 654, loss: 6121.810818\nEPOCH: 655, loss: 7855.641870\nEPOCH: 656, loss: 7817.827397\nEPOCH: 657, loss: 1134.096642\nEPOCH: 658, loss: 4563.456473\nEPOCH: 659, loss: 2199.520670\nEPOCH: 660, loss: 1035.251877\nEPOCH: 661, loss: 5787.783010\nEPOCH: 662, loss: 5584.755941\nEPOCH: 663, loss: 8032.216095\nEPOCH: 664, loss: 5572.308770\nEPOCH: 665, loss: 3554.385247\nEPOCH: 666, loss: 16243.170559\nEPOCH: 667, loss: 1689.464938\nEPOCH: 668, loss: 2783.493535\nEPOCH: 669, loss: 4074.139079\nEPOCH: 670, loss: 477.307205\nEPOCH: 671, loss: 3177.917166\nEPOCH: 672, loss: 3839.010337\nEPOCH: 673, loss: 2353.907072\nEPOCH: 674, loss: 5173.251338\nEPOCH: 675, loss: 2682.951979\nEPOCH: 676, loss: 3601.003704\nEPOCH: 677, loss: 2655.115548\nEPOCH: 678, loss: 2511.912618\nEPOCH: 679, loss: 2165.833814\nEPOCH: 680, loss: 2657.303529\nEPOCH: 681, loss: 1548.182846\nEPOCH: 682, loss: 1718.386877\nEPOCH: 683, loss: 5630.660054\nEPOCH: 684, loss: 3571.040362\nEPOCH: 685, loss: 3537.594264\nEPOCH: 686, loss: 8298.638726\nEPOCH: 687, loss: 2769.553786\nEPOCH: 688, loss: 987.363576\nEPOCH: 689, loss: 6207.838272\nEPOCH: 690, loss: 1442.237059\nEPOCH: 691, loss: 943.746300\nEPOCH: 692, loss: 1180.090937\nEPOCH: 693, loss: 3679.094167\nEPOCH: 694, loss: 4069.882627\nEPOCH: 695, loss: 9786.506962\nEPOCH: 696, loss: 20331.203006\nEPOCH: 697, loss: 1638.139691\nEPOCH: 698, loss: 9184.695953\nEPOCH: 699, loss: 1972.293691\nEPOCH: 700, loss: 4866.409568\nEPOCH: 701, loss: 7923.023621\nEPOCH: 702, loss: 2138.031194\nEPOCH: 703, loss: 1980.124155\nEPOCH: 704, loss: 3717.367598\nEPOCH: 705, loss: 6375.200464\nEPOCH: 706, loss: 10358.043607\nEPOCH: 707, loss: 1474.197533\nEPOCH: 708, loss: 4448.840574\nEPOCH: 709, loss: 2566.279596\nEPOCH: 710, loss: 1394.304375\nEPOCH: 711, loss: 2145.569518\nEPOCH: 712, loss: 2426.247865\nEPOCH: 713, loss: 2877.989314\nEPOCH: 714, loss: 1160.829337\nEPOCH: 715, loss: 3305.711646\nEPOCH: 716, loss: 1353.748137\nEPOCH: 717, loss: 1936.706789\nEPOCH: 718, loss: 3027.047308\nEPOCH: 719, loss: 3762.618749\nEPOCH: 720, loss: 7243.370905\nEPOCH: 721, loss: 4821.261418\nEPOCH: 722, loss: 1684.716978\nEPOCH: 723, loss: 2236.601986\nEPOCH: 724, loss: 16889.487900\nEPOCH: 725, loss: 629.254037\nEPOCH: 726, loss: 1192.224010\nEPOCH: 727, loss: 2675.125991\nEPOCH: 728, loss: 6533.241778\nEPOCH: 729, loss: 1320.663472\nEPOCH: 730, loss: 3059.662270\nEPOCH: 731, loss: 2080.387237\nEPOCH: 732, loss: 4695.003485\nEPOCH: 733, loss: 5740.149810\nEPOCH: 734, loss: 1953.564344\nEPOCH: 735, loss: 3587.830296\nEPOCH: 736, loss: 2846.863770\nEPOCH: 737, loss: 1251.583053\nEPOCH: 738, loss: 1250.341270\nEPOCH: 739, loss: 815.030275\nEPOCH: 740, loss: 5744.688123\nEPOCH: 741, loss: 1870.749836\nEPOCH: 742, loss: 320.266184\nEPOCH: 743, loss: 1513.207721\nEPOCH: 744, loss: 3626.961447\nEPOCH: 745, loss: 6994.904018\nEPOCH: 746, loss: 1970.188546\nEPOCH: 747, loss: 1968.882150\nEPOCH: 748, loss: 3287.192500\nEPOCH: 749, loss: 4789.142912\nEPOCH: 750, loss: 181.639099\nEPOCH: 751, loss: 1444.157659\nEPOCH: 752, loss: 2118.161109\nEPOCH: 753, loss: 2426.024056\nEPOCH: 754, loss: 4051.566368\nEPOCH: 755, loss: 2465.115554\nEPOCH: 756, loss: 5450.951805\nEPOCH: 757, loss: 5444.527231\nEPOCH: 758, loss: 3496.464089\nEPOCH: 759, loss: 2168.638110\nEPOCH: 760, loss: 2821.405745\nEPOCH: 761, loss: 5765.599326\nEPOCH: 762, loss: 3478.248703\nEPOCH: 763, loss: 1279.539051\nEPOCH: 764, loss: 1796.454640\nEPOCH: 765, loss: 5204.403288\nEPOCH: 766, loss: 5350.891746\nEPOCH: 767, loss: 5540.309337\nEPOCH: 768, loss: 3211.104625\nEPOCH: 769, loss: 4589.853725\nEPOCH: 770, loss: 1459.295280\nEPOCH: 771, loss: 2030.345296\nEPOCH: 772, loss: 12350.266049\nEPOCH: 773, loss: 1447.152546\nEPOCH: 774, loss: 5778.794974\nEPOCH: 775, loss: 6882.369698\nEPOCH: 776, loss: 3080.294809\nEPOCH: 777, loss: 3959.510120\nEPOCH: 778, loss: 2192.539838\nEPOCH: 779, loss: 6785.972147\nEPOCH: 780, loss: 1937.226885\nEPOCH: 781, loss: 6366.782515\nEPOCH: 782, loss: 8721.239011\nEPOCH: 783, loss: 3312.115519\nEPOCH: 784, loss: 5900.166258\nEPOCH: 785, loss: 3794.583915\nEPOCH: 786, loss: 3602.693129\nEPOCH: 787, loss: 2467.966843\nEPOCH: 788, loss: 3673.748197\nEPOCH: 789, loss: 960.747956\nEPOCH: 790, loss: 1423.690659\nEPOCH: 791, loss: 452.080356\nEPOCH: 792, loss: 2577.260866\nEPOCH: 793, loss: 408.426240\nEPOCH: 794, loss: 2743.212414\nEPOCH: 795, loss: 5349.332502\nEPOCH: 796, loss: 1536.665071\nEPOCH: 797, loss: 3914.158527\nEPOCH: 798, loss: 491.552431\nEPOCH: 799, loss: 2343.356554\nEPOCH: 800, loss: 17326.682851\nEPOCH: 801, loss: 1542.277122\nEPOCH: 802, loss: 2178.522194\nEPOCH: 803, loss: 1335.796255\nEPOCH: 804, loss: 6907.811724\nEPOCH: 805, loss: 3904.929267\nEPOCH: 806, loss: 1927.516204\nEPOCH: 807, loss: 1442.501710\nEPOCH: 808, loss: 1848.746185\nEPOCH: 809, loss: 2915.684223\nEPOCH: 810, loss: 5398.199523\nEPOCH: 811, loss: 1515.953822\nEPOCH: 812, loss: 8370.880120\nEPOCH: 813, loss: 1499.913826\nEPOCH: 814, loss: 1271.780820\nEPOCH: 815, loss: 10954.503717\nEPOCH: 816, loss: 2989.699715\nEPOCH: 817, loss: 625.406352\nEPOCH: 818, loss: 2118.442339\nEPOCH: 819, loss: 2075.602983\nEPOCH: 820, loss: 1913.015977\nEPOCH: 821, loss: 1243.391317\nEPOCH: 822, loss: 2205.115031\nEPOCH: 823, loss: 2253.871811\nEPOCH: 824, loss: 1593.008549\nEPOCH: 825, loss: 3091.566993\nEPOCH: 826, loss: 629.866870\nEPOCH: 827, loss: 690.646055\nEPOCH: 828, loss: 11248.084100\nEPOCH: 829, loss: 1894.375068\nEPOCH: 830, loss: 3081.970156\nEPOCH: 831, loss: 3876.012734\nEPOCH: 832, loss: 1197.466416\nEPOCH: 833, loss: 1709.823716\nEPOCH: 834, loss: 3280.039368\nEPOCH: 835, loss: 4634.339308\nEPOCH: 836, loss: 2569.226426\nEPOCH: 837, loss: 1821.310635\nEPOCH: 838, loss: 2428.712121\nEPOCH: 839, loss: 1417.713146\nEPOCH: 840, loss: 4252.452865\nEPOCH: 841, loss: 1278.737589\nEPOCH: 842, loss: 2373.785544\nEPOCH: 843, loss: 4185.110940\nEPOCH: 844, loss: 3699.542396\nEPOCH: 845, loss: 2635.647422\nEPOCH: 846, loss: 19518.334215\nEPOCH: 847, loss: 2003.289565\nEPOCH: 848, loss: 4103.099215\nEPOCH: 849, loss: 2451.278170\nEPOCH: 850, loss: 1325.093747\nEPOCH: 851, loss: 3715.617769\nEPOCH: 852, loss: 1066.555030\nEPOCH: 853, loss: 2377.190727\nEPOCH: 854, loss: 20735.845937\nEPOCH: 855, loss: 1717.260121\nEPOCH: 856, loss: 3725.176409\nEPOCH: 857, loss: 1548.135827\nEPOCH: 858, loss: 2063.277563\nEPOCH: 859, loss: 3919.230284\nEPOCH: 860, loss: 994.580536\nEPOCH: 861, loss: 2114.478950\nEPOCH: 862, loss: 3051.839502\nEPOCH: 863, loss: 2447.137331\nEPOCH: 864, loss: 2105.735808\nEPOCH: 865, loss: 1313.193596\nEPOCH: 866, loss: 2410.659919\nEPOCH: 867, loss: 3745.886928\nEPOCH: 868, loss: 1309.732943\nEPOCH: 869, loss: 4432.452508\nEPOCH: 870, loss: 1696.710942\nEPOCH: 871, loss: 1556.861432\nEPOCH: 872, loss: 2335.371777\nEPOCH: 873, loss: 4473.158667\nEPOCH: 874, loss: 4489.495100\nEPOCH: 875, loss: 532.863518\nEPOCH: 876, loss: 2698.303454\nEPOCH: 877, loss: 2469.466750\nEPOCH: 878, loss: 5882.508832\nEPOCH: 879, loss: 1622.345859\nEPOCH: 880, loss: 3412.329519\nEPOCH: 881, loss: 1907.763176\nEPOCH: 882, loss: 4899.630754\nEPOCH: 883, loss: 3009.477958\nEPOCH: 884, loss: 507.387539\nEPOCH: 885, loss: 768.700275\nEPOCH: 886, loss: 3578.721429\nEPOCH: 887, loss: 2912.626331\nEPOCH: 888, loss: 3909.561618\nEPOCH: 889, loss: 3274.214204\nEPOCH: 890, loss: 3741.751532\nEPOCH: 891, loss: 5287.565151\nEPOCH: 892, loss: 3435.790271\nEPOCH: 893, loss: 1358.477994\nEPOCH: 894, loss: 989.690686\nEPOCH: 895, loss: 1921.182531\nEPOCH: 896, loss: 2480.330516\nEPOCH: 897, loss: 5140.225444\nEPOCH: 898, loss: 2994.491626\nEPOCH: 899, loss: 1119.848999\nEPOCH: 900, loss: 475.872355\nEPOCH: 901, loss: 2121.668645\nEPOCH: 902, loss: 2788.102144\nEPOCH: 903, loss: 4067.902178\nEPOCH: 904, loss: 3324.812833\nEPOCH: 905, loss: 4547.881423\nEPOCH: 906, loss: 3905.690669\nEPOCH: 907, loss: 2508.162269\nEPOCH: 908, loss: 2860.164610\nEPOCH: 909, loss: 19359.890187\nEPOCH: 910, loss: 3162.225939\nEPOCH: 911, loss: 4447.452231\nEPOCH: 912, loss: 3032.835205\nEPOCH: 913, loss: 1738.255949\nEPOCH: 914, loss: 2263.444541\nEPOCH: 915, loss: 1271.530044\nEPOCH: 916, loss: 4410.205534\nEPOCH: 917, loss: 8574.898701\nEPOCH: 918, loss: 3035.909870\nEPOCH: 919, loss: 5058.263699\nEPOCH: 920, loss: 1034.211473\nEPOCH: 921, loss: 946.032101\nEPOCH: 922, loss: 1950.484890\nEPOCH: 923, loss: 2544.753300\nEPOCH: 924, loss: 18383.363136\nEPOCH: 925, loss: 2612.290801\nEPOCH: 926, loss: 1741.741489\nEPOCH: 927, loss: 19934.716944\nEPOCH: 928, loss: 2917.001233\nEPOCH: 929, loss: 2179.017259\nEPOCH: 930, loss: 1260.676006\nEPOCH: 931, loss: 5364.912921\nEPOCH: 932, loss: 1012.798136\nEPOCH: 933, loss: 2117.553557\nEPOCH: 934, loss: 5659.449738\nEPOCH: 935, loss: 5743.370635\nEPOCH: 936, loss: 14759.159170\nEPOCH: 937, loss: 4137.298557\nEPOCH: 938, loss: 9827.461162\nEPOCH: 939, loss: 3597.809658\nEPOCH: 940, loss: 4907.132474\nEPOCH: 941, loss: 13196.808551\nEPOCH: 942, loss: 6804.450492\nEPOCH: 943, loss: 4352.954195\nEPOCH: 944, loss: 1638.769948\nEPOCH: 945, loss: 8570.626005\nEPOCH: 946, loss: 3048.474405\nEPOCH: 947, loss: 14467.881329\nEPOCH: 948, loss: 2954.017192\nEPOCH: 949, loss: 2460.644215\nEPOCH: 950, loss: 1480.750908\nEPOCH: 951, loss: 3416.681674\nEPOCH: 952, loss: 3297.163971\nEPOCH: 953, loss: 2168.311274\nEPOCH: 954, loss: 1409.764997\nEPOCH: 955, loss: 2588.202273\nEPOCH: 956, loss: 4174.382118\nEPOCH: 957, loss: 4082.714622\nEPOCH: 958, loss: 3295.983264\nEPOCH: 959, loss: 4233.589534\nEPOCH: 960, loss: 353.847951\nEPOCH: 961, loss: 3122.353817\nEPOCH: 962, loss: 2455.834051\nEPOCH: 963, loss: 1256.895362\nEPOCH: 964, loss: 1506.541518\nEPOCH: 965, loss: 6790.092246\nEPOCH: 966, loss: 978.194469\nEPOCH: 967, loss: 1868.342350\nEPOCH: 968, loss: 8600.120068\nEPOCH: 969, loss: 2746.944437\nEPOCH: 970, loss: 15631.806824\nEPOCH: 971, loss: 4708.148644\nEPOCH: 972, loss: 3257.167279\nEPOCH: 973, loss: 1881.184792\nEPOCH: 974, loss: 11436.900234\nEPOCH: 975, loss: 4510.404477\nEPOCH: 976, loss: 3681.919699\nEPOCH: 977, loss: 2011.775714\nEPOCH: 978, loss: 3907.364491\nEPOCH: 979, loss: 3804.964699\nEPOCH: 980, loss: 15377.857788\nEPOCH: 981, loss: 2693.812779\nEPOCH: 982, loss: 2578.516499\nEPOCH: 983, loss: 922.901894\nEPOCH: 984, loss: 2653.427348\nEPOCH: 985, loss: 3219.458671\nEPOCH: 986, loss: 1389.841424\nEPOCH: 987, loss: 5328.564390\nEPOCH: 988, loss: 2288.788374\nEPOCH: 989, loss: 1326.527080\nEPOCH: 990, loss: 5245.745812\nEPOCH: 991, loss: 13648.721440\nEPOCH: 992, loss: 3362.020262\nEPOCH: 993, loss: 1198.685204\nEPOCH: 994, loss: 3214.016337\nEPOCH: 995, loss: 4975.883137\nEPOCH: 996, loss: 1417.244781\nEPOCH: 997, loss: 4280.574169\nEPOCH: 998, loss: 2638.495863\nEPOCH: 999, loss: 4562.867942\n"
    }
   ],
   "source": [
    "# numpy version\n",
    "\n",
    "w = np.random.rand(X_train.shape[0])\n",
    "k = 10\n",
    "v = np.random.rand(X_train.shape[0], k)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.00001263\n",
    "\n",
    "PRINT_STEP = 1\n",
    "\n",
    "EPOCH = 1000\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X_train.shape[1], size=BATCH_SIZE)\n",
    "    X_batch = X_train[:, index]\n",
    "    y_batch = y_train[index]\n",
    "\n",
    "    # linear part\n",
    "    linear_part = np.dot(w.T, X_batch)\n",
    "\n",
    "    # cross part\n",
    "    cross_part = np.zeros(BATCH_SIZE)\n",
    "    for m in range(0, X_train.shape[0] - 1):\n",
    "        for n in range(m + 1, X_train.shape[0]):\n",
    "            v_m = v[m, :]\n",
    "            v_n = v[n, :]\n",
    "            cross_part += np.dot(v_m, v_n) * np.multiply(X_batch[m, :], X_batch[n, :])\n",
    " \n",
    "    y_hat = linear_part + cross_part\n",
    "    loss = y_hat - y_batch\n",
    "\n",
    "    # linear pard update grade\n",
    "    w = w - LEARNING_RATE * np.multiply(loss, X_batch).sum(axis=1) / BATCH_SIZE\n",
    "    \n",
    "    # matrix grad update\n",
    "    for p in range(X_train.shape[0]):\n",
    "        for u in range(k):\n",
    "            v_grad = np.multiply(loss,  X_batch[p, :]**2 * (v[:, u].sum() -  v[p, u])).sum()\n",
    "            v[p, u] =  v[p, u] - LEARNING_RATE * v_grad / BATCH_SIZE\n",
    "        \n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, (loss * loss).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Version\n",
    "\n",
    "import torch"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}