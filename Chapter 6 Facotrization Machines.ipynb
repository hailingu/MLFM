{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine\n",
    "\n",
    "之前的　Linear Regression 的形式是形如　$\\hat{y}= w^T x$，Factorization Machine 在　Linear Regression　的基础之上添加了所谓的交叉项，即　$c_{ij}x_ix_j$, 即　$\\hat{y} = w_1x_1 + \\ldots + w_fx_f + \\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q$，由于有些交叉项在实际中并不存在，所以使用向量相乘的办法，用一个　$f \\times k$　的矩阵，从中任选两个向量 $v_i, v_j$　相乘作为系数，从　$n$　个向量中任选两个相乘构成的系数个数一共有 $C_f^2 = \\frac{f(f-1)}{2}$　个，刚好等于后面交叉项的数量。\n",
    "\n",
    "最后的交叉项可以写成　$\\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q = \\frac{1}{2}(\\sum_{p=1}^f \\sum_{q=1}^f c_{pq}x_px_q - \\sum_p^f v_p^2x_p^2)=\\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)(\\sum_{q=1}^f v_{q,u}x_q) - (\\sum_{p=1}^f v_{p, u}^2x_p^2)]= \\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)^2- (\\sum_{p=1}^f v_{p, u}^2x_p^2)]$，降低计算复杂度。\n",
    "\n",
    "其中 $c_{pq} = v_p \\times v_q$\n",
    "\n",
    "计算　Loss 采用的函数仍是　MSE，即　$loss = \\frac{1}{2} \\sum_i^n(\\hat{y}_i - y_i)^2$\n",
    "\n",
    "Gradient 的计算及更新：\n",
    "\n",
    "$\\begin{align*}\n",
    "w_i & = w_i - \\eta \\cdot [\\sum_i^n x_i \\cdot (\\hat{y_i} - y_i) ] \\\\\n",
    "v_{p,u} & = v_{p,u} - \\eta \\cdot \\sum_{i=1}^n [(\\hat{y_i} - y_i) \\cdot (x_{i, p} (\\sum_{p=1}^f v_{p, u} x_{i, p}) - x_{i, p}^2v_{p, u})]\n",
    "\\end{align*}$\n",
    "\n",
    "Factorization Machine 是一个适用于回归场景的算法。为了演示这个算法，采用典型的　Boston Housing 的数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load boston housing \n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "\n",
    "boston_df = pd.DataFrame(X, columns=boston.feature_names)\n",
    "\n",
    "# z-score\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].mean()) / boston_df[feature].std()\n",
    "\n",
    "# min-max\n",
    "for feature in boston.feature_names:\n",
    "    boston_df[feature] = (boston_df[feature] - boston_df[feature].min()) / (boston_df[feature].max() - boston_df[feature].min())\n",
    "\n",
    "X = np.c_[boston_df.values, np.ones(X.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 673.563069\nEPOCH: 15, loss: 956.476021\nEPOCH: 30, loss: 957.301041\nEPOCH: 45, loss: 1131.980154\nEPOCH: 60, loss: 754.626841\nEPOCH: 75, loss: 875.519168\nEPOCH: 90, loss: 1088.979308\nEPOCH: 105, loss: 851.566699\nEPOCH: 120, loss: 633.100518\nEPOCH: 135, loss: 974.367024\nEPOCH: 150, loss: 788.498512\nEPOCH: 165, loss: 618.190015\nEPOCH: 180, loss: 955.684439\nEPOCH: 195, loss: 963.548392\nEPOCH: 210, loss: 943.783070\nEPOCH: 225, loss: 931.921670\nEPOCH: 240, loss: 636.592666\nEPOCH: 255, loss: 891.015543\nEPOCH: 270, loss: 1016.814411\nEPOCH: 285, loss: 879.597780\n"
    }
   ],
   "source": [
    "k = 9\n",
    "\n",
    "LEARNING_RATE = 1e-6\n",
    "EPOCH = 300\n",
    "\n",
    "BATCH_SIZE = 30\n",
    "\n",
    "PRINT_NUMS = 20\n",
    "PRINT_INTERVAL = EPOCH / PRINT_NUMS\n",
    "\n",
    "f = X.shape[1] - 1\n",
    "\n",
    "w = np.random.uniform(0, 1, size=(1, f + 1))\n",
    "v = np.random.uniform(0, 1, size=(f, k))\n",
    "\n",
    "n = BATCH_SIZE\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X.shape[0], size=BATCH_SIZE)\n",
    "    sample_x = X[index]\n",
    "    sample_y = y[index]\n",
    "\n",
    "    # linear part\n",
    "    linear_part = np.dot(w, sample_x.T)\n",
    "\n",
    "    # cross part\n",
    "    p = np.dot(v.T, sample_x[:,0:f].T)\n",
    "    p = p * p\n",
    "    p = np.sum(p, axis=0)\n",
    "\n",
    "    q = np.dot(np.multiply(v.T, v.T), np.multiply(sample_x[:, 0:f].T, sample_x[:, 0:f].T))\n",
    "    q = np.sum(q, axis=0)\n",
    "    r = (p - q).reshape(1, q.shape[0])\n",
    "\n",
    "    y_hat = linear_part  + r\n",
    "    loss = y_hat - sample_y\n",
    "    # update gradient\n",
    "    w = w - LEARNING_RATE * np.dot(loss, sample_x)\n",
    "\n",
    " \n",
    "    for p in range(f):\n",
    "        for u in range(k):\n",
    "            sumation = 0\n",
    "            for i in range(n):\n",
    "                sumation2 = 0\n",
    "                for q in range(f):\n",
    "                    sumation2 += v[q, u] * sample_x[i, q]\n",
    "                sumation += sample_x[i, p] * (sumation2 - sample_x[i, p] * v[p, u]) * loss[0, i]\n",
    "        v[p, u] -= LEARNING_RATE * sumation\n",
    "\n",
    "    if epoch % PRINT_INTERVAL == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, loss.sum()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python361064bit5f34a3a27dba4e5b962d1d44305a0d2a",
   "display_name": "Python 3.6.10 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}