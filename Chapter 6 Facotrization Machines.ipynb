{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Factorization Machine\n",
    "\n",
    "之前的　Linear Regression 的形式是形如　$\\hat{y}= w^T x$，Factorization Machine 在　Linear Regression　的基础之上添加了所谓的交叉项，即　$c_{ij}x_ix_j$, 即　$\\hat{y} = w_1x_1 + \\ldots + w_fx_f + \\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q$，由于有些交叉项在实际中并不存在，所以使用向量相乘的办法，用一个　$f \\times k$　的矩阵，从中任选两个向量 $v_i, v_j$　相乘作为系数，从　$n$　个向量中任选两个相乘构成的系数个数一共有 $C_f^2 = \\frac{f(f-1)}{2}$　个，刚好等于后面交叉项的数量。\n",
    "\n",
    "最后的交叉项可以写成　$\\sum_{p=1}^{f-1} \\sum_{q=p+1}^{f} c_{pq}x_px_q = \\frac{1}{2}(\\sum_{p=1}^f \\sum_{q=1}^f c_{pq}x_px_q - \\sum_p^f v_p^2x_p^2)=\\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)(\\sum_{q=1}^f v_{q,u}x_q) - (\\sum_{p=1}^f v_{p, u}^2x_p^2)]= \\frac{1}{2} \\sum_{u=1}^k[(\\sum_{p=1}^fv_{p,u}x_p)^2- (\\sum_{p=1}^f v_{p, u}^2x_p^2)]$，降低计算复杂度。\n",
    "\n",
    "其中 $c_{pq} = v_p \\cdot v_q$\n",
    "\n",
    "计算　Loss 采用的函数仍是　MSE，即　$loss = \\frac{1}{2} \\sum_i^n(\\hat{y}_i - y_i)^2$\n",
    "\n",
    "Gradient 的计算及更新：\n",
    "\n",
    "$\\begin{align*}\n",
    "w_i & = w_i - \\eta \\cdot \\frac{1}{n} \\cdot [\\sum_i^n x_i \\cdot (\\hat{y_i} - y_i) ] \\\\\n",
    "v_{p,u} & = v_{p,u} - \\eta \\cdot \\frac{1}{n} \\cdot \\sum_{i=1}^n [(\\hat{y_i} - y_i) \\cdot x_{i, p}^2 \\cdot (\\sum_{p=1}^f v_{p, u} - v_{p, u})]\n",
    "\\end{align*}$\n",
    "\n",
    "Factorization Machine 是一个适用于回归场景的算法（分类场景也可以使用）。为了演示这个算法，采用 Boston Housing 数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "boston = load_boston()\n",
    "boston_data = pd.DataFrame(boston.data,  columns=boston.feature_names)\n",
    "boston_data['bias'] = np.ones(boston.data.shape[0])\n",
    "boston_data['target'] = boston.target\n",
    "\n",
    "ss = StandardScaler()\n",
    "boston_data = ss.fit_transform(boston_data)\n",
    "\n",
    "shape = boston_data.shape\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston_data[0:shape[0], 0:-1], boston_data[0:shape[0], -1], test_size=0.25,\n",
    "                                                    random_state=33)\n",
    "\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 4079.220222\nEPOCH: 1, loss: 5909.605041\nEPOCH: 2, loss: 3003.988646\nEPOCH: 3, loss: 55614.314832\nEPOCH: 4, loss: 2662.843102\nEPOCH: 5, loss: 12014.136855\nEPOCH: 6, loss: 23536.633983\nEPOCH: 7, loss: 398885.922581\nEPOCH: 8, loss: 67933995.477794\nEPOCH: 9, loss: 15787614749033836314624.000000\n"
    }
   ],
   "source": [
    "# numpy version\n",
    "\n",
    "w = np.random.rand(X_train.shape[0])\n",
    "k = 6\n",
    "v = np.random.rand(X_train.shape[0], k)\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "EPOCH = 20\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X_train.shape[1], size=BATCH_SIZE)\n",
    "    X_batch = X_train[:, index]\n",
    "    y_batch = y_train[index]\n",
    "\n",
    "    # linear part\n",
    "    linear_part = np.dot(w.T, X_batch)\n",
    "\n",
    "    # cross part\n",
    "    cross_part = np.zeros(BATCH_SIZE)\n",
    "    for m in range(0, X_train.shape[0] - 1):\n",
    "        for n in range(m + 1, X_train.shape[0]):\n",
    "            v_m = v[m, :]\n",
    "            v_n = v[n, :]\n",
    "            cross_part += np.dot(v_m, v_n) * np.multiply(X_batch[m, :], X_batch[n, :])\n",
    " \n",
    "    y_hat = linear_part + cross_part\n",
    "    loss = y_hat - y_batch\n",
    "\n",
    "    # linear pard update grade\n",
    "    w = w - LEARNING_RATE * np.multiply(loss, X_batch).sum(axis=1) / BATCH_SIZE\n",
    "    \n",
    "    # matrix grad update\n",
    "    for p in range(X_train.shape[0]):\n",
    "        for u in range(k):\n",
    "            v_grad = np.multiply(loss,  X_batch[p, :]**2 * (v[:, u].sum() -  v[p, u])).sum()\n",
    "            v[p, u] =  v[p, u] - LEARNING_RATE * v_grad / BATCH_SIZE\n",
    "        \n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, (loss**2).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 2504.345184\nEPOCH: 2, loss: 935.662576\nEPOCH: 4, loss: 215.542112\nEPOCH: 6, loss: 559.341540\nEPOCH: 8, loss: 589.773573\nEPOCH: 10, loss: 62.043805\nEPOCH: 12, loss: 115.991072\nEPOCH: 14, loss: 413.338862\nEPOCH: 16, loss: 21.422009\nEPOCH: 18, loss: 21.678856\n"
    }
   ],
   "source": [
    "# PyTorch Version\n",
    "\n",
    "import torch\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.double\n",
    "\n",
    "INPUT_DIMENSION, OUTPUT_DIMENSION = X_train.shape[0], 1\n",
    "w = torch.randn(INPUT_DIMENSION, OUTPUT_DIMENSION, device=device, dtype=dtype, requires_grad=True)\n",
    "k = 6\n",
    "v = torch.randn(INPUT_DIMENSION, k, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "EPOCH = 20\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X_train.shape[0], size=BATCH_SIZE)\n",
    "    X_batch = torch.from_numpy(X_train[:, index]).reshape(INPUT_DIMENSION, BATCH_SIZE)\n",
    "    y_batch = torch.from_numpy(y_train[index])\n",
    "\n",
    "    # linear part\n",
    "    linear_part = w.T.mm(X_batch)\n",
    "\n",
    "    # cross part\n",
    "    cross_part = torch.from_numpy(np.zeros(BATCH_SIZE)).reshape((1,-1))\n",
    "    for m in range(0, X_train.shape[0] - 1):\n",
    "        for n in range(m + 1, X_train.shape[0]):\n",
    "            v_m = v[m, :].reshape((1, -1))\n",
    "            v_n = v[n, :].reshape((1, -1))\n",
    "            cross_part += v_m.mul(v_n).sum() * X_batch[m, :].mul(X_batch[n, :])\n",
    " \n",
    "    y_hat = linear_part + cross_part\n",
    "    loss = ((y_hat - y_batch)**2).sum() / 2\n",
    "    loss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        w -= LEARNING_RATE * w.grad\n",
    "        v -= LEARNING_RATE * v.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        w.grad.zero_()\n",
    "        v.grad.zero_()\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1594300981412",
   "display_name": "Python 3.6.8 64-bit ('mlfm': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}