{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Field-aware Factorization Machines\n",
    "\n",
    "在 Factorization Machines 的基础之上，做了些许修改。在 Factorization Machines 中，每一个特征 $f$ 对应唯一的向量 $f_i$，特征交叉的时候就是直接与另一个特征对应的向量 $f_j$ 点乘后作为交叉特征的系数。但是在 Field-aware Factorization Machines 中，对这种特征交叉做了一些修改，把特征划分到不同的 Field 上（假如有 n 个特征，然后把它们划分到了 $f$ 个域上，每个特征对每个域有一个 $k$ 维的向量，交叉项系数带来的参数总量是 $n \\times f \\times k$)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import os\n",
    "\n",
    "BASEDIR = os.getcwd()\n",
    "\n",
    "features = []\n",
    "fields = []\n",
    "values = []\n",
    "y_train = []\n",
    "field_cnt = -1\n",
    "feature_cnt = -1\n",
    "with open(BASEDIR + '/assets/datasets/criteo_ctr/small_train.txt') as f:\n",
    "    line = f.readline()\n",
    "    line = line.strip('\\n')\n",
    "    while line:\n",
    "        elems = line.split(' ')\n",
    "        y_train.append(int(elems[0]))\n",
    "        tmp_feature_idx = []\n",
    "        tmp_field_idx = []\n",
    "        tmp_feature_value = []\n",
    "        for i in range(1, len(elems)):\n",
    "            field, feature, value = elems[i].split(':')\n",
    "            field_cnt = max(field_cnt, int(field))\n",
    "            feature_cnt = max(feature_cnt, int(feature))\n",
    "            tmp_feature_idx.append([0, int(feature)])\n",
    "            tmp_field_idx.append(int(field))\n",
    "            tmp_feature_value.append(float(value))\n",
    "        features.append(tmp_feature_idx)\n",
    "        fields.append(tmp_field_idx)\n",
    "        values.append(tmp_feature_value)\n",
    "        line = f.readline()\n",
    "        line = line.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 20.670942\nEPOCH: 1, loss: 19.604090\nEPOCH: 2, loss: 18.563363\nEPOCH: 3, loss: 17.547062\nEPOCH: 4, loss: 16.553460\nEPOCH: 5, loss: 15.581000\nEPOCH: 6, loss: 14.628138\nEPOCH: 7, loss: 13.693390\nEPOCH: 8, loss: 12.775317\nEPOCH: 9, loss: 11.872528\n"
    }
   ],
   "source": [
    "# PyTorch Version\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.double\n",
    "\n",
    "X_train = []\n",
    "for feature, field, value in zip(features, fields, values):\n",
    "    feature.append([0, feature_cnt])\n",
    "    value.append(0.0)\n",
    "    X_train.append({'feature': feature, 'value': value, 'field': field})\n",
    "\n",
    "INPUT_DIMENSION, OUTPUT_DIMENSION = feature_cnt + 1, 1\n",
    "w = torch.rand(INPUT_DIMENSION, OUTPUT_DIMENSION, device=device, dtype=dtype, requires_grad=True)\n",
    "k = 5\n",
    "cv = torch.rand(feature_cnt + 1, field_cnt + 1, k, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "LEARNING_RATE = 1e-1\n",
    "\n",
    "EPOCH = 10\n",
    "PRINT_STEP = EPOCH / 10\n",
    "N = len(y_train)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    start = 0\n",
    "    end = start + BATCH_SIZE\n",
    "    while start < N:\n",
    "        if end >= N:\n",
    "            end = N\n",
    "\n",
    "        X_batch = torch.empty(feature_cnt + 1, end - start, dtype=torch.double)\n",
    "        y_batch = torch.from_numpy(np.array(y_train[start:end], np.double)).reshape(-1, end - start)\n",
    "        for idx in range(end - start):\n",
    "            i = torch.LongTensor(X_train[start:end][idx]['feature'])\n",
    "            v = torch.DoubleTensor(X_train[start:end][idx]['value'])\n",
    "            X_batch[:, idx] = torch.sparse.DoubleTensor(i.t(), v).to_dense()\n",
    "\n",
    "        linear_part = w.T.mm(X_batch)\n",
    "        cross_part = torch.zeros(1, end - start, dtype=torch.double, requires_grad=False)\n",
    "\n",
    "        for idx in range(end - start):\n",
    "            x = X_train[start:end][idx]\n",
    "            for f1 in range(0, len(x['field']) - 1):\n",
    "                for f2 in range(f1 + 1, len(x['field'])):\n",
    "                    f1_feature = x['feature'][f1][1]\n",
    "                    f2_feature = x['feature'][f2][1]\n",
    "\n",
    "                    f1_field = x['field'][f1]\n",
    "                    f2_field = x['field'][f2]\n",
    "\n",
    "                    factor = cv[f1_feature, f2_field, :].mul(cv[f2_feature, f1_field, :])\n",
    "                    cross_part[0, idx] += factor.sum() * x['value'][f1] * x['value'][f2]\n",
    "        y_hat = linear_part + cross_part\n",
    "        y_hat = 1.0 / (1.0 + torch.exp(-1 * y_hat))\n",
    "\n",
    "        logloss = -1 * torch.sum(\n",
    "            torch.mul(y_batch, torch.log(y_hat)) + torch.mul((1 - y_batch), torch.log(1 - y_hat))) / BATCH_SIZE\n",
    "        logloss.backward()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            w -= LEARNING_RATE * w.grad\n",
    "            cv -= LEARNING_RATE * cv.grad\n",
    "\n",
    "            # Manually zero the gradients after updating weights\n",
    "            w.grad.zero_()\n",
    "            cv.grad.zero_()\n",
    "\n",
    "        start = end\n",
    "        end = start + BATCH_SIZE\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, logloss))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python36864bitmlfmcondafca915a6e1ae4fb7ab8b19f2bd50bf32",
   "language": "python",
   "display_name": "Python 3.6.8 64-bit ('mlfm': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}