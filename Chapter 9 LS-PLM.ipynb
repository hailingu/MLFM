{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LS-PLM(MLR)\n",
    "\n",
    "MLR 的思想是，先考虑数据 $x$ 属于整体的哪一个部分，再看这个数据在这个部分上的二分类结果。MLR 的数学表达式可以很好的体现这个思想：$f(x) = \\sum_{i=1}^m \\frac{e^{u_i \\cdot x}}{\\sum_{j=1}^{m} e^{u_j \\cdot x}} \\cdot \\frac{1}{1 + e^{-w_i \\cdot x}}$ 。利用 PyTorch，可以很容易实现这个端到端的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "BASEDIR = os.getcwd()\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "y = []\n",
    "\n",
    "col_cnt = -1\n",
    "\n",
    "idx = 0\n",
    "with open(BASEDIR + '/assets/datasets/criteo_ctr/small_train.txt') as f:\n",
    "    line = f.readline()\n",
    "    line = line.strip('\\n')\n",
    "    while line:\n",
    "        elems = line.split(' ')\n",
    "        y.append(int(elems[0]))\n",
    "        for i in range(1, len(elems)):\n",
    "            field, feature, value = elems[i].split(':')\n",
    "            col_cnt = max(col_cnt, int(feature))\n",
    "            row.append(idx)\n",
    "            col.append(int(feature))\n",
    "            data.append(float(value))\n",
    "            \n",
    "        line = f.readline()\n",
    "        idx += 1\n",
    "\n",
    "i = torch.LongTensor([row, col])\n",
    "v = torch.DoubleTensor(data)\n",
    "X_train = torch.sparse.DoubleTensor(i, v).to_dense().T\n",
    "y_train = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 1.989119\nEPOCH: 1, loss: 1.625195\nEPOCH: 2, loss: 2.060608\nEPOCH: 3, loss: 2.718789\nEPOCH: 4, loss: 2.496257\nEPOCH: 5, loss: 1.947505\nEPOCH: 6, loss: 1.977964\nEPOCH: 7, loss: 2.236859\nEPOCH: 8, loss: 2.670226\nEPOCH: 9, loss: 2.287574\n"
    }
   ],
   "source": [
    "# PyTorch Version\n",
    "\n",
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + torch.exp(-1 * x))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.double\n",
    "\n",
    "INPUT_DIMENSION, OUTPUT_DIMENSION = X_train.shape[0], 1\n",
    "\n",
    "m = 3\n",
    "u = torch.rand(INPUT_DIMENSION, m, device=device, dtype=dtype, requires_grad=True)\n",
    "w = torch.rand(INPUT_DIMENSION, m, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X_train.shape[1], size=BATCH_SIZE)\n",
    "    X_batch = X_train[:, index]\n",
    "    y_batch = y_train[index]\n",
    "\n",
    "    y_softmax_part = torch.exp(u.T.mm(X_batch))\n",
    "    y_linear_part = sigmoid(w.T.mm(X_batch))\n",
    "\n",
    "    y_hat = y_softmax_part.mul(y_linear_part).div(y_softmax_part.sum(axis=0)).sum(axis=0)\n",
    "    logloss = -1 * torch.sum(torch.mul(y_batch, torch.log(y_hat)) + torch.mul((1 - y_batch), torch.log(1 - y_hat))) / BATCH_SIZE\n",
    "\n",
    "    logloss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u -= LEARNING_RATE * u.grad\n",
    "        w -= LEARNING_RATE * w.grad\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, logloss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class LSPLM(nn.Module):\n",
    "    def __init__(self, m, optimizer, penalty='l2', batch_size=32, epoch=100, learning_rate=0.1, verbose=False):\n",
    "        super(LSPLM, self).__init__()\n",
    "        self.m = m\n",
    "        self.optimizer = optimizer\n",
    "        self.batch_size = batch_size\n",
    "        self.epoch = epoch\n",
    "        self.verbose = verbose\n",
    "        self.learning_rate = learning_rate\n",
    "        self.penalty = penalty\n",
    "\n",
    "        self.softmax = None\n",
    "        self.logistic = None\n",
    "\n",
    "        self.loss_fn = nn.BCELoss(reduction='mean')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.softmax is None and self.logistic is None:\n",
    "            self.softmax = nn.Sequential(\n",
    "                nn.Linear(X.shape[1], self.m).double(),\n",
    "                nn.Softmax(dim=1).double()\n",
    "            )\n",
    "\n",
    "            self.logistic = nn.Sequential(\n",
    "                nn.Linear(X.shape[1], self.m, bias=True).double()\n",
    "                , nn.Sigmoid())\n",
    "\n",
    "            if self.optimizer == 'Adam':\n",
    "                self.optimizer = optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "            elif self.optimizer == 'SGD':\n",
    "                self.optimizer = optim.SGD(self.parameters(), lr=self.learning_rate, weight_decay=1e-5, momentum=0.1,\n",
    "                                           nesterov=True)\n",
    "\n",
    "        # noinspection DuplicatedCode\n",
    "        for epoch in range(self.epoch):\n",
    "\n",
    "            start = 0\n",
    "            end = start + self.batch_size\n",
    "            while start < X.shape[0]:\n",
    "\n",
    "                if end >= X.shape[0]:\n",
    "                    end = X.shape[0]\n",
    "\n",
    "                X_batch = torch.from_numpy(X[start:end, :])\n",
    "                y_batch = torch.from_numpy(y[start:end]).reshape(1, end - start)\n",
    "\n",
    "                y_batch_pred = self.forward(X_batch).reshape(1, end - start)\n",
    "                loss = self.loss_fn(y_batch_pred, y_batch)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                start = end\n",
    "                end += self.batch_size\n",
    "\n",
    "            if self.verbose and epoch % (self.epoch / 20) == 0:\n",
    "                print('EPOCH: %d, loss: %f' % (epoch, loss))\n",
    "        return self\n",
    "\n",
    "    def forward(self, X):\n",
    "        logistic_out = self.logistic(X)\n",
    "        softmax_out = self.softmax(X)\n",
    "        combine_out = logistic_out.mul(softmax_out)\n",
    "        return combine_out.sum(1)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        return self.forward(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = torch.from_numpy(X)\n",
    "        out = self.forward(X)\n",
    "        out[out >= 0.5] = 1.0\n",
    "        out[out < 0.5] = 0.0\n",
    "        return out\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('mlfm': conda)",
   "language": "python",
   "name": "python36864bitmlfmcondafca915a6e1ae4fb7ab8b19f2bd50bf32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}