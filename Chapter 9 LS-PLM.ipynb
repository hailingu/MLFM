{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# LS-PLM(MLR)\n",
    "\n",
    "MLR 的思想是，先考虑数据 $x$ 属于整体的哪一个部分，再看这个数据在这个部分上的二分类结果。MLR 的数学表达式可以很好的体现这个思想：$f(x) = \\sum_{i=1}^m \\frac{e^{u_i \\cdot x}}{\\sum_{j=1}^{m} e^{u_j \\cdot x}} \\cdot \\frac{1}{1 + e^{-w_i \\cdot x}}$ 。利用 PyTorch，可以很容易实现这个端到端的模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "BASEDIR = os.getcwd()\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "y = []\n",
    "\n",
    "col_cnt = -1\n",
    "\n",
    "idx = 0\n",
    "with open(BASEDIR + '/assets/datasets/criteo_ctr/small_train.txt') as f:\n",
    "    line = f.readline()\n",
    "    line = line.strip('\\n')\n",
    "    while line:\n",
    "        elems = line.split(' ')\n",
    "        y.append(int(elems[0]))\n",
    "        for i in range(1, len(elems)):\n",
    "            field, feature, value = elems[i].split(':')\n",
    "            col_cnt = max(col_cnt, int(feature))\n",
    "            row.append(idx)\n",
    "            col.append(int(feature))\n",
    "            data.append(float(value))\n",
    "            \n",
    "        line = f.readline()\n",
    "        idx += 1\n",
    "\n",
    "i = torch.LongTensor([row, col])\n",
    "v = torch.DoubleTensor(data)\n",
    "X_train = torch.sparse.DoubleTensor(i, v).to_dense().T\n",
    "y_train = torch.LongTensor(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "EPOCH: 0, loss: 1.989119\nEPOCH: 1, loss: 1.625195\nEPOCH: 2, loss: 2.060608\nEPOCH: 3, loss: 2.718789\nEPOCH: 4, loss: 2.496257\nEPOCH: 5, loss: 1.947505\nEPOCH: 6, loss: 1.977964\nEPOCH: 7, loss: 2.236859\nEPOCH: 8, loss: 2.670226\nEPOCH: 9, loss: 2.287574\n"
    }
   ],
   "source": [
    "# PyTorch Version\n",
    "\n",
    "import torch\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1 + torch.exp(-1 * x))\n",
    "\n",
    "device = torch.device('cpu')\n",
    "dtype = torch.double\n",
    "\n",
    "INPUT_DIMENSION, OUTPUT_DIMENSION = X_train.shape[0], 1\n",
    "\n",
    "m = 3\n",
    "u = torch.rand(INPUT_DIMENSION, m, device=device, dtype=dtype, requires_grad=True)\n",
    "w = torch.rand(INPUT_DIMENSION, m, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "EPOCH = 10\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "PRINT_STEP = EPOCH / 10\n",
    "\n",
    "for epoch in range(EPOCH):\n",
    "    index = np.random.randint(0, X_train.shape[1], size=BATCH_SIZE)\n",
    "    X_batch = X_train[:, index]\n",
    "    y_batch = y_train[index]\n",
    "\n",
    "    y_softmax_part = torch.exp(u.T.mm(X_batch))\n",
    "    y_linear_part = sigmoid(w.T.mm(X_batch))\n",
    "\n",
    "    y_hat = y_softmax_part.mul(y_linear_part).div(y_softmax_part.sum(axis=0)).sum(axis=0)\n",
    "    logloss = -1 * torch.sum(torch.mul(y_batch, torch.log(y_hat)) + torch.mul((1 - y_batch), torch.log(1 - y_hat))) / BATCH_SIZE\n",
    "\n",
    "    logloss.backward()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        u -= LEARNING_RATE * u.grad\n",
    "        w -= LEARNING_RATE * w.grad\n",
    "\n",
    "    if epoch % PRINT_STEP == 0:\n",
    "        print('EPOCH: %d, loss: %f' % (epoch, logloss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('mlfm': conda)",
   "language": "python",
   "name": "python36864bitmlfmcondafca915a6e1ae4fb7ab8b19f2bd50bf32"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}