# Model Selection

> 在现实任务中，我们往往有多种学习算法可供选择，甚至对同一个学习算法，当使用不同的参数配置时，可会产生不同模型，那么，我们该选用哪一个学习算法，使用哪一种参数配置呢？这就是机器学习中的“模型选择”（model selection）问题。
——周志华 机器学习

模型的预测输出与样本的真实输出之间的差异称为“误差”，模型在训练集上的误差称为“训练误差”或“经验误差”，在新样本上的误差称为“泛化误差”，我们最终的目的是希望训练出一个泛化误差小的模型出来，理想的解决方案是获取候选模型的泛化误差，然后选择泛化误差最小的模型，然而由于我们事先并不知道新样本是什么样的，无法直接获得泛化误差，而训练误差又由于过拟合现象的存在而不适合作为标准，因此通常使用模型在测试集上的测试误差作为泛化误差的近似，测试集尽量与训练集互斥，否则可能得到一个偏“乐观”的估计

假设容量为n 的全部样本集：$$ D=\{(x_1,y_1),(x_2,y_2),\ldots,(x_n,y_n)\}$$ 划分为互斥的两个集合 $$Train =\{(x_1,y_1),(x_2,y_2),\ldots,(x_m,y_m)\}$$ ，容量为 $$m$$，和 $$Test=\{(x_1,y_1),(x_2,y_2),\ldots,(x_t,y_t)\}$$ ，容量为 $$t$$，$$Train$$ 用于模型的训练评估、选择、调参，$$Test$$ 用于测试模型实际的泛化能力（精度、召回率等）。还有一种方式把原始数据集划分成 3 个集合，$$Train$$、$$Dev$$、$$Test$$，其中 $$Train$$ 用来训练模型， $$Dev$$ 用来调整参数， $$Test$$ 用来验证模型的效果。划分数据集的时候，要尽可能保持的

划分要尽可能保持数据分布的一致性，例如在分类任务中，至少要保持样本的类别比例相近，单次划分得到的结果往往不太可靠，可多次随机划分，重复试验，取测试误差的平均值作为泛化误差的估计。在小规模的数据集上一般使用 (0.6,0.4) 的比例划分，如果数据训练集很大，超过 100 万，可以考虑使用 (0.98,0.02) 这样的比例划分。

# K-fold Cross-Validation

将数据集 $$D$$ 划分为 $$k$$ 个大小相似的互斥子集， $$D_1,D_2,\ldots,D_k$$ ，每次用 $$k−1$$ 个子集的并集作为训练集，剩余的那个子集作为验证集，进行 $$k$$ 次训练和测试，将 $$k$$ 个测试误差的平均值作为泛化误差的估计。

# Leave-One-Out Cross Validation

从数据集  $$D$$ 中挑出一个数据用于测试，其他的数据用于构建模型，这样做的话，如果数据集的大小为 $$k$$ 需要做 $$k$$ 次训练和测试，和 K-fold Cross-Validation 一样，将 $$k$$ 个测试误差的平均值作为泛化误差的估计。

# Bootstrap

前面处理的训练集的样本容量比 $$m$$ 小，会引入一些由样本规模不同而导致的估计偏差。自助法通过随机有放回采样 $$m$$ 次，得到容量为 $$m$$ 的数据集 $$D$$（有部分重复样本），没有被采到的样本比例约为 $$ \lim \limits_{m \to \infty } {(1 - \frac1m)^m} = \frac1e \approx 0.368$$，将 $$D$$ 作为训练集，没被采到的样本作为验证集。

这种做法在数据集较小，难以有效划分训练集和验证集的情况下很有用，方便产生多个不同的训练集，但是改变了初始数据集的分布，会引入估计偏差。
